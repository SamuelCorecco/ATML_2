{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset serves more to help the model ‘work’ better, we decided to test our approach but on another dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this for kaggle (missing library)\n",
    "the reset of kernel is because some thimes we have problem with BitsAndBytesConfig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:11:14.378606Z",
     "iopub.status.busy": "2024-12-19T11:11:14.378218Z",
     "iopub.status.idle": "2024-12-19T11:11:43.937876Z",
     "shell.execute_reply": "2024-12-19T11:11:43.936835Z",
     "shell.execute_reply.started": "2024-12-19T11:11:14.378572Z"
    },
    "id": "HUhPN-pSOeR4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:11:53.049951Z",
     "iopub.status.busy": "2024-12-19T11:11:53.049591Z",
     "iopub.status.idle": "2024-12-19T11:12:13.031815Z",
     "shell.execute_reply": "2024-12-19T11:12:13.030873Z",
     "shell.execute_reply.started": "2024-12-19T11:11:53.049917Z"
    },
    "id": "BaUZ5PTqO3Dl",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samu/Desktop/Usi/Advanced Topics in Machine Learning/ATML_2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import lightning as L\n",
    "import gc\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "We need to extract and prepare the dataset and build alpaca. <br>\n",
    "So we need to extract some information that we then want to get from the model i.e. Release Date, Gender and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-19T11:12:13.033535Z",
     "iopub.status.busy": "2024-12-19T11:12:13.033172Z",
     "iopub.status.idle": "2024-12-19T11:12:14.887127Z",
     "shell.execute_reply": "2024-12-19T11:12:14.886210Z",
     "shell.execute_reply.started": "2024-12-19T11:12:13.033506Z"
    },
    "id": "q8NFV_FJO25H",
    "outputId": "c9cd0677-c220-4314-a7a0-6ff3afb59deb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean len 434.1848624774142\n",
      "This anime was released on Apr 3, 1998 to Apr 24, 1999.\n",
      "Its genres are Action, Award Winning, Sci-Fi.\n",
      "The description of this anime is: Crime is timeless. By the year 2071, humanity has expanded across the galaxy, filling the surface of other planets with settlements like those on Earth. These new societies are plagued by murder, drug use, and theft, and intergalactic outlaws are hunted by a growing number of tough bounty hunters.  Spike Spiegel and Jet Black pursue criminals throughout space to make a humble living. Beneath his goofy and aloof demeanor, Spike is haunted by the weight of his violent past. Meanwhile, Jet manages his own troubled memories while taking care of Spike and the Bebop, their ship. The duo is joined by the beautiful con artist Faye Valentine, odd child Edward Wong Hau Pepelu Tivrusky IV, and Ein, a bioengineered Welsh Corgi.  While developing bonds and working to catch a colorful cast of criminals, the Bebop crew's lives are disrupted by a menace from Spike's past. As a rival's maniacal plot continues to unravel, Spike must choose between life with his newfound family or revenge for his old wounds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET_PATH = \"/kaggle/input/animedata/anime-dataset-2023.csv\"\n",
    "\n",
    "\n",
    "\n",
    "anime_df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "\n",
    "def prepare_alpaca_format_strict(df):\n",
    "    tot_len = 0\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        instruction = \"Describe this anime\"\n",
    "        input_text = row['Name']\n",
    "        release_date = row['Aired']\n",
    "        genres = row['Genres']\n",
    "        synopsis = row['Synopsis']\n",
    "\n",
    "        if pd.isnull(input_text) or pd.isnull(release_date) or pd.isnull(genres) or pd.isnull(synopsis):\n",
    "            continue\n",
    "\n",
    "        synopsis = synopsis.replace(\"\\n\", \" \")\n",
    "\n",
    "        output_text = (f\"This anime was released on {release_date}.\\n\"\n",
    "                       f\"Its genres are {genres}.\\n\"\n",
    "                       f\"The description of this anime is: {synopsis}\")\n",
    "\n",
    "        tot_len += len(output_text)\n",
    "\n",
    "        example = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        }\n",
    "        examples.append(example)\n",
    "\n",
    "    print(\"mean len\", tot_len/len(examples))\n",
    "    return examples\n",
    "\n",
    "alpaca_dataset_name_only = prepare_alpaca_format_strict(anime_df)\n",
    "print(alpaca_dataset_name_only[0][\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "To train the model, we did QLoRa as for part 2, masked the prompt part in the label and trained the model to understand the structure of the response. \n",
    "<br><br>\n",
    "After training the loss did not change any more so we guessed that the model is not able to learn the descriptions of the cores. This is due to the fact that 1 the model is relatively small and obviously has not been trained to understand the content of the cores. This is why we decided to carry out one more step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783,
     "referenced_widgets": [
      "ed30c6498b654199bc89b96876ec4627",
      "7fc10a6dfb59447194202429c0952f3b",
      "21ca13cefb65444692efacc93fb989b3",
      "9982171fd0b04fd49942dee62dcd681b",
      "a19f5b79088d49eb8a3b5869ae068a60",
      "09196afb77e44c7393c0d10c91838723",
      "a5d5cf6e76444ede8713b0cc5c685d35",
      "c30a61b097da42cab87874e5ba941db0",
      "11547e358d8d4e81b42cfc4197350e06",
      "ff0a060929544eefacb8c6cee5dcde47",
      "f0bf9d219ec3437bbc73378690d6c89f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-19T08:30:21.475125Z",
     "iopub.status.busy": "2024-12-19T08:30:21.474764Z",
     "iopub.status.idle": "2024-12-19T10:31:47.262339Z",
     "shell.execute_reply": "2024-12-19T10:31:47.261542Z",
     "shell.execute_reply.started": "2024-12-19T08:30:21.475092Z"
    },
    "id": "nE2bUvLHO2pl",
    "outputId": "cd5742ce-ed9b-4ccc-bfb1-b94b7d06d029",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241219_083027-tvftivsg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/tvftivsg' target=\"_blank\">anime_lama_1</a></strong> to <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/tvftivsg' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/tvftivsg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf05b040c98d4bf281842dcdf0a70393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bcf2097e204849956e1677ef835418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0394e454c4a9453ab625d44693aa652d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a8e36faacc4a0ea7203ddd2a473fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cee5b5360646fda6bd8ed909561c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e23c92dafb94d89be6a0f4a281a87cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fbba25b62d4f08aca1d5972789a5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cab9310df0e41e8b5f5f24ef05c8ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 616 M  | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "615 M     Non-trainable params\n",
      "616 M     Total params\n",
      "2,466.947 Total estimated model params size (MB)\n",
      "442       Modules in train mode\n",
      "315       Modules in eval mode\n",
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297c2e7e25344f5c99083c2052189dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='910.221 MB of 990.619 MB uploaded\\r'), FloatProgress(value=0.9188401720308857, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr><tr><td>train_loss_step</td><td>▄▇▇▇█▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▄▅▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_loss</td><td>1.00491</td></tr><tr><td>train_loss_epoch</td><td>1.50064</td></tr><tr><td>train_loss_step</td><td>0.81289</td></tr><tr><td>trainer/global_step</td><td>311</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">anime_lama_1</strong> at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/tvftivsg' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/tvftivsg</a><br/> View project at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241219_083027-tvftivsg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"anime_Lama\", \n",
    "    name=\"anime_lama_1\",\n",
    "    config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "alpaca_data = alpaca_dataset_name_only\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Imposta il dispositivo su CUDA\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Usa la CPU se CUDA non è disponibile\n",
    "    print(\"CUDA not available, using CPU.\")\n",
    "    \n",
    "train_formatted = [f\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\" for sample in alpaca_data]\n",
    "train_dataset = Dataset.from_dict({\"text\": train_formatted})\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 Quantization Configuration\n",
    "# And Model and Tokenizer Loading\n",
    "# ####################################\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ####################################\n",
    "# STEP 3  LoRa\n",
    "# ####################################\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ####################################\n",
    "# STEP 4: DataLoader\n",
    "# ####################################\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Tokenizza il testo\n",
    "    inputs = tokenizer(\n",
    "        [sample['text'] for sample in batch], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=400\n",
    "    )\n",
    "    # Clona input_ids per usarli come etichette\n",
    "    labels = inputs.input_ids.clone()\n",
    "    \n",
    "    # Applica maschera per ignorare la parte di istruzione e input\n",
    "    for i, sample in enumerate(batch):\n",
    "        text = sample['text']\n",
    "        # Trova l'indice di inizio della risposta\n",
    "        response_start = text.find(\"### Response:\")\n",
    "        if response_start != -1:\n",
    "            response_start_token_idx = tokenizer(text[:response_start], truncation=True, max_length=450, return_tensors=\"pt\")[\"input_ids\"].size(1)\n",
    "            labels[i, :response_start_token_idx] = -100  # Maschera l'istruzione e l'input\n",
    "    \n",
    "    # Trasferisci i tensori sul dispositivo\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "    labels = labels.to(device)\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    batch_size=10,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# AT this point with an input \n",
    "\n",
    "# Cowboy Bebop\n",
    "# ### Response:\n",
    "# This anime was released on Apr 3, 1998 to Apr 24, 1999.\n",
    "# Its genres are Action, Award Winning, Sci-Fi.\n",
    "# The description of this ...\n",
    "\n",
    "# We have a label that is\n",
    "\n",
    "# </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>### Response:\n",
    "# This anime was released on Apr 3, 1998 to Apr 24, 1999.\n",
    "# Its genres are Action, Award Winning, Sci-Fi.\n",
    "# The description of this ...\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 5 Lightning Wrapper\n",
    "# ####################################\n",
    "\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # Shift logits and labels\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        # Compute LM loss\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # Log loss to wandb\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model)\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Trainer + Train\n",
    "# ####################################\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints\",\n",
    "    filename=\"finetuned_model-{epoch:02d}-{train_loss:.2f}\",\n",
    "    save_top_k=-1,\n",
    "    save_last=True,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "\n",
    "# Configura il logger di wandb\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"nome_progetto\",  # Nome del progetto wandb\n",
    "    log_model=True  # Salva i checkpoint del modello su wandb\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,  # Aggiungi il logger\n",
    "    accumulate_grad_batches=8,\n",
    "    precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Save the Fine-tuned Model\n",
    "# ####################################\n",
    "\n",
    "model.save_pretrained(\"./finetuned_qlora_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_qlora_model\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 2\n",
    "We train the model with longer descriptions but only for famous anime, to see if the approach is able to teach something about anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:13:35.135845Z",
     "iopub.status.busy": "2024-12-19T11:13:35.134880Z",
     "iopub.status.idle": "2024-12-19T11:13:35.503941Z",
     "shell.execute_reply": "2024-12-19T11:13:35.503010Z",
     "shell.execute_reply.started": "2024-12-19T11:13:35.135805Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use only a total of: 343 Anime to teach te model some information about anime\n",
      "mean len 1009.1341107871721\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET_PATH = \"/kaggle/input/animedata/anime-dataset-2023.csv\"\n",
    "anime_df = pd.read_csv(DATASET_PATH)\n",
    "anime_df = anime_df[anime_df['Score'] != '']\n",
    "anime_df = anime_df[anime_df['Score'] != 'UNKNOWN']\n",
    "anime_df = anime_df[anime_df['Score'].apply(lambda x: float(x) > 8.2)]\n",
    "print(f\"We use only a total of: {len(anime_df)} Anime to teach te model some information about anime\")\n",
    "alpaca_dataset_name_only = prepare_alpaca_format_strict(anime_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:13:45.005888Z",
     "iopub.status.busy": "2024-12-19T11:13:45.005522Z",
     "iopub.status.idle": "2024-12-19T12:38:51.128635Z",
     "shell.execute_reply": "2024-12-19T12:38:51.127756Z",
     "shell.execute_reply.started": "2024-12-19T11:13:45.005857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241219_111349-7841a060</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/7841a060' target=\"_blank\">anime_lama_2</a></strong> to <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/7841a060' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/7841a060</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf6ee5caf9f45d390052e3e37e8d4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e56b28e025846358847ef1d7f34140a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fbba3639bc4a5f81619c0b2a2061b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d0f99319dd498c921b046149129266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c68857edc164460ad3c07811786f21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1166aa148a904258b12cafadcd7e4b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66566d73e66a454989815ab93c9836a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c949aa18a613425ba893fa20834c7560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85/1263063852.py:101: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")  # Carica il checkpoint\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 616 M  | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "615 M     Non-trainable params\n",
      "616 M     Total params\n",
      "2,466.947 Total estimated model params size (MB)\n",
      "442       Modules in train mode\n",
      "315       Modules in eval mode\n",
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c4413936bd4924819ca2225efe2b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='132.425 MB of 990.620 MB uploaded\\r'), FloatProgress(value=0.13367852264420008, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▃▃▂▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▁▂▁▁▂▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▆▆▂█▁▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss</td><td>1.77236</td></tr><tr><td>train_loss_epoch</td><td>1.6586</td></tr><tr><td>train_loss_step</td><td>1.66346</td></tr><tr><td>trainer/global_step</td><td>359</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">anime_lama_2</strong> at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/7841a060' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama/runs/7841a060</a><br/> View project at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/anime_Lama</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241219_111349-7841a060/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"anime_Lama\", \n",
    "    name=\"anime_lama_2\",  \n",
    "    config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 10,\n",
    "        \"epochs\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# ####################################\n",
    "# STEP 2: Configurazione del Dispositivo\n",
    "# ####################################\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available, using CPU.\")\n",
    "\n",
    "# ####################################\n",
    "# STEP 3: Caricamento Nuovo Dataset\n",
    "# ####################################\n",
    "alpaca_data = alpaca_dataset_name_only  # Nuovo dataset già pronto\n",
    "\n",
    "train_formatted = [\n",
    "    f\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\"\n",
    "    for sample in alpaca_data\n",
    "]\n",
    "train_dataset = Dataset.from_dict({\"text\": train_formatted})\n",
    "\n",
    "# ####################################\n",
    "# STEP 4: DataLoader\n",
    "# ####################################\n",
    "def collate_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        [sample['text'] for sample in batch], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=800\n",
    "    )\n",
    "    labels = inputs.input_ids.clone()\n",
    "    for i, sample in enumerate(batch):\n",
    "        text = sample['text']\n",
    "        response_start = text.find(\"### Response:\")\n",
    "        if response_start != -1:\n",
    "            response_start_token_idx = tokenizer(\n",
    "                text[:response_start], truncation=True, max_length=400, return_tensors=\"pt\"\n",
    "            )[\"input_ids\"].size(1)\n",
    "            labels[i, :response_start_token_idx] = -100\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "    labels = labels.to(device)\n",
    "    return inputs, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    batch_size=5,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# ####################################\n",
    "# STEP 5: Caricamento del Checkpoint Manualmente\n",
    "# ####################################\n",
    "import torch\n",
    "\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Caricamento del modello e setup originale\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Caricamento manuale dello stato dal checkpoint\n",
    "checkpoint_path = \"./checkpoints/last.ckpt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")  # Carica il checkpoint\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)  # Carica lo stato del modello\n",
    "\n",
    "# Wrappa il modello aggiornato in PyTorch Lightning\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model)\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Trainer e Continuazione del Training\n",
    "# ####################################\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     dirpath=\"./checkpoints\",\n",
    "#     filename=\"finetuned_model-{epoch:02d}-{train_loss:.2f}\",\n",
    "#     save_top_k=-1,\n",
    "#     save_last=True,\n",
    "#     monitor=\"train_loss\",\n",
    "#     mode=\"min\"\n",
    "# )\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"anime_Lama\", \n",
    "    name=\"anime_lama_continued\", \n",
    "    log_model=True\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    accumulate_grad_batches=8,\n",
    "    precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=40,\n",
    "    # callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Continua il training dal checkpoint\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "# ####################################\n",
    "# STEP 7: Salvataggio Finale del Modello\n",
    "# ####################################\n",
    "model.save_pretrained(\"./finetuned_qlora_model_updated\")\n",
    "tokenizer.save_pretrained(\"./finetuned_qlora_model_updated\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "On the mac we can test the trained models and see how they perform and see if there are any improvements\n",
    "<br>\n",
    "Spoiler: yes, the trained model for the second time was able to understand more things although the description is still inaccurate. We saw that it occasionally recognises characters and themes but it would need much more training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: This anime was released on Apr 4, 1997 to Oct 25, 1997.\n",
      "Its genres are Action, Sci-Fi.\n",
      "The description of this anime is: In the year 2039, a group of scientists at Japan's Ibuki Research Center discover a mysterious device that can create sentient life in any form. The scientist who created it, Shinji Ikada, is given a mission by the government to destroy humanity and create a new world dominated by machines as their own country begins to fall apart. This mission will have dire consequences for those who oppose him, including his former students Kōsuke\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "torch.cuda.empty_cache() \n",
    "torch.mps.empty_cache() \n",
    "\n",
    "device = torch.device(\"mps\") \n",
    "\n",
    "model_path = \"Models/Anime_step_1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "def generate_response(instruction, query, model, tokenizer, max_new_tokens=150, temperature=0.7, top_p=0.95):\n",
    "    # Build prompt\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{query}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Output:\n",
    "    # for this we use the sampling method this to have non deterministic results\n",
    "    # we have (whit current parameter) more real result and less random result\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,             \n",
    "        temperature=temperature,   \n",
    "        top_p=top_p,                \n",
    "        repetition_penalty=1.2,     \n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "instruction = \"Describe this anime\"\n",
    "query = \"Neon Genesis Evangelion\"\n",
    "\n",
    "response = generate_response(instruction, query, model, tokenizer)\n",
    "print(\"out:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: This anime was released on Jan 23, 1995 to Mar 07, 1996.\n",
      "Its genres are Action, Adventure, Sci-Fi.\n",
      "The description of this anime is: The young and powerful Shinji Ikari has been given a mission by the mysterious Rebuilding Council—to destroy humanity and eradicate all life in order to prevent mankind from becoming evil. With his brother G\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "torch.cuda.empty_cache() \n",
    "torch.mps.empty_cache() \n",
    "\n",
    "device = torch.device(\"mps\") \n",
    "\n",
    "model_path = \"Models/Anime_Model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "def generate_response(instruction, query, model, tokenizer, max_new_tokens=100, temperature=0.7, top_p=0.95):\n",
    "    # Build prompt\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{query}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Output:\n",
    "    # for this we use the sampling method this to have non deterministic results\n",
    "    # we have (whit current parameter) more real result and less random result\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,             \n",
    "        temperature=temperature,   \n",
    "        top_p=top_p,                \n",
    "        repetition_penalty=1.2,     \n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "instruction = \"Describe this anime\"\n",
    "query = \"Neon Genesis Evangelion\"\n",
    "\n",
    "response = generate_response(instruction, query, model, tokenizer)\n",
    "print(\"out:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6329616,
     "sourceId": 10236246,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09196afb77e44c7393c0d10c91838723": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11547e358d8d4e81b42cfc4197350e06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "21ca13cefb65444692efacc93fb989b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c30a61b097da42cab87874e5ba941db0",
      "max": 6227,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_11547e358d8d4e81b42cfc4197350e06",
      "value": 20
     }
    },
    "7fc10a6dfb59447194202429c0952f3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09196afb77e44c7393c0d10c91838723",
      "placeholder": "​",
      "style": "IPY_MODEL_a5d5cf6e76444ede8713b0cc5c685d35",
      "value": "Epoch 0:   0%"
     }
    },
    "9982171fd0b04fd49942dee62dcd681b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff0a060929544eefacb8c6cee5dcde47",
      "placeholder": "​",
      "style": "IPY_MODEL_f0bf9d219ec3437bbc73378690d6c89f",
      "value": " 20/6227 [00:42&lt;3:40:58,  0.47it/s, v_num=2]"
     }
    },
    "a19f5b79088d49eb8a3b5869ae068a60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "a5d5cf6e76444ede8713b0cc5c685d35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c30a61b097da42cab87874e5ba941db0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed30c6498b654199bc89b96876ec4627": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7fc10a6dfb59447194202429c0952f3b",
       "IPY_MODEL_21ca13cefb65444692efacc93fb989b3",
       "IPY_MODEL_9982171fd0b04fd49942dee62dcd681b"
      ],
      "layout": "IPY_MODEL_a19f5b79088d49eb8a3b5869ae068a60"
     }
    },
    "f0bf9d219ec3437bbc73378690d6c89f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff0a060929544eefacb8c6cee5dcde47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
