{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import viridis\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Investigate dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, we will analyse the dataset in order to fully understand its properties and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  ### Human: Can you write a short introduction ...\n",
      "1  ### Human: ¬øCUales son las etapas del desarrol...\n",
      "2  ### Human: Can you explain contrastive learnin...\n",
      "3  ### Human: I want to start doing astrophotogra...\n",
      "4  ### Human: M√©todo del Perceptr√≥n bicl√°sico: de...\n",
      "\n",
      "Example first Row (train): ### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog\n",
      "Example first Row (test): ### Human: –ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ —è–∑—ã–∫–µ swift, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–∞—Å—Å–∏–≤ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –∞ –∑–∞—Ç–µ–º –≤—ã–≤–æ–¥–∏—Ç –µ–≥–æ –Ω–∞ —ç–∫—Ä–∞–Ω### Assistant: –í–æ—Ç —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–∞—Å—Å–∏–≤ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –∏ –≤—ã–≤–æ–¥–∏—Ç –µ–≥–æ –Ω–∞ —ç–∫—Ä–∞–Ω:\n",
      "\n",
      "```swift\n",
      "func sortAndPrintArray(_ array: [Int]) {\n",
      "  // –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –º–∞—Å—Å–∏–≤–∞, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π\n",
      "  var sortedArray = array\n",
      "  // –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–∞—Å—Å–∏–≤ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é\n",
      "  sortedArray.sort()\n",
      "  // –í—ã–≤–æ–¥–∏–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ –Ω–∞ —ç–∫—Ä–∞–Ω\n",
      "  print(sortedArray)\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "–¢—ã –º–æ–∂–µ—à—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É —Ñ—É–Ω–∫—Ü–∏–∏, –≤—ã–∑–≤–∞–≤ –µ–µ —Å –ª—é–±—ã–º –º–∞—Å—Å–∏–≤–æ–º —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n",
      "\n",
      "```swift\n",
      "sortAndPrintArray([5, 2, 7, 9, -3])\n",
      "```\n",
      "\n",
      "–ù–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–∏—Ç—Å—è:\n",
      "\n",
      "```sh\n",
      "[-3, 2, 5, 7, 9]\n",
      "```\n",
      "---\n",
      "–ù–∞–¥–µ—é—Å—å, —á—Ç–æ —è —Å–º–æ–≥ —Ç–µ–±–µ –ø–æ–º–æ—á—å. –ù—É–∂–Ω–æ –ª–∏ —Ç–µ–±–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –µ—â—ë? üòä\n",
      "Number of Rows total:     10364\n",
      "Number of Rows test:      518\n",
      "Number of Rows train:     9846\n"
     ]
    }
   ],
   "source": [
    "# PRINT SOME INFO\n",
    "print(df_train.head(5))\n",
    "print(\"\")\n",
    "print(\"Example first Row (train):\", df_train.iloc[0][\"text\"])\n",
    "print(\"Example first Row (test):\", df_test.iloc[0][\"text\"])\n",
    "print(\"Number of Rows total:    \", len(df_train) + len(df_test))\n",
    "print(\"Number of Rows test:     \", len(df_test))\n",
    "print(\"Number of Rows train:    \", len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can extract other interesting parameters such as how many languages exist within and how they are distributed. To do this, we need to use existing libraries such as langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n"
     ]
    }
   ],
   "source": [
    "def plot_language_distribution(df):\n",
    "    def detect_language(text):\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return \"unknown\"\n",
    "\n",
    "    df[\"language\"] = df[\"text\"].apply(detect_language)\n",
    "    language_distribution = df[\"language\"].value_counts().reset_index()\n",
    "    language_distribution.columns = [\"language\", \"count\"]\n",
    "\n",
    "    total_languages = language_distribution[\"language\"].nunique()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(language_distribution[\"language\"], language_distribution[\"count\"], color=plt.cm.tab20.colors)\n",
    "    plt.title(f\"Language Distribution in Dataset (Total Languages: {total_languages})\", fontsize=16)\n",
    "    plt.xlabel(\"Language\", fontsize=14)\n",
    "    plt.ylabel(\"Count\", fontsize=14)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Total Languages:\", total_languages)\n",
    "\n",
    "print(\"Train Dataset\")\n",
    "plot_language_distribution(df_train)\n",
    "print(\"Test Dataset\")\n",
    "plot_language_distribution(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another thing we can analyse are the lengths e.g. how long on average the users' sentences are, how long the Assistant replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "mean len for human:         122.01,      std: 248.68\n",
      "mean len for Assistant:     1000.23,     std: 810.76\n",
      "\n",
      "Test dataset:\n",
      "mean len for human:         124.71,      std: 173.26\n",
      "mean len for Assistant:     1014.85,     std: 886.90\n"
     ]
    }
   ],
   "source": [
    "def len_computation(df):\n",
    "    def extract_human_and_assistant(text):\n",
    "        try:\n",
    "            human = text.split(\"### Assistant:\")[0].replace(\"### Human:\", \"\").strip()\n",
    "            assistant = text.split(\"### Assistant:\")[1].strip()\n",
    "            return human, assistant\n",
    "        except IndexError:\n",
    "            return \"\", \"\"\n",
    "        \n",
    "    df[\"human\"], df[\"assistant\"] = zip(*df[\"text\"].apply(extract_human_and_assistant))\n",
    "\n",
    "    df[\"human_length\"] = df[\"human\"].apply(len)\n",
    "    df[\"assistant_length\"] = df[\"assistant\"].apply(len)\n",
    "\n",
    "    human_mean = df[\"human_length\"].mean()\n",
    "    human_std = df[\"human_length\"].std()\n",
    "\n",
    "    assistant_mean = df[\"assistant_length\"].mean()\n",
    "    assistant_std = df[\"assistant_length\"].std()\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(f\"mean len for human:         {human_mean:.2f},      std: {human_std:.2f}\")\n",
    "    print(f\"mean len for Assistant:     {assistant_mean:.2f},     std: {assistant_std:.2f}\")\n",
    "\n",
    "print(\"Train dataset:\")\n",
    "len_computation(df_train)\n",
    "print(\"\")\n",
    "print(\"Test dataset:\")\n",
    "len_computation(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can extract the dictionary, see the number of words. To get an idea we want to see the number of different words for the whole train dataset and all words for English only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in train dataset: 155992\n",
      "Number of unique words in train dataset (only english): 45492\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_dictionary(df, language=None):\n",
    "    def clean_text(text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "    df['clean_text'] = df['text'].fillna('').apply(clean_text)\n",
    "\n",
    "    word_counts = Counter()\n",
    "\n",
    "    for text in df['clean_text']:\n",
    "        if language is None or detect(text) == language:\n",
    "            word_counts.update(text.split())\n",
    "\n",
    "    return dict(word_counts)\n",
    "\n",
    "\n",
    "dictionary_train = extract_dictionary(df_train)\n",
    "print(\"Number of unique words in train dataset:\", len(dictionary_train))\n",
    "\n",
    "dictionary_train_en = extract_dictionary(df_train, language=\"en\")\n",
    "print(\"Number of unique words in train dataset (only english):\", len(dictionary_train_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test just to get an idea of how many words in English there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in train dataset (only english) after stemming:                                  34557\n",
      "Number of unique words in train dataset (only english) after lemmatizing:                               34484\n",
      "Number of unique words in train dataset (only english) after removing numbers and non-alphabetic words: 27535\n"
     ]
    }
   ],
   "source": [
    "# try to remove the word that can be lemmed\n",
    "stemmer = PorterStemmer()\n",
    "dictionary_train_en = {stemmer.stem(word): count for word, count in dictionary_train_en.items()}\n",
    "print(\"Number of unique words in train dataset (only english) after stemming:                                 \", len(dictionary_train_en))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dictionary_train_en = {lemmatizer.lemmatize(word): count for word, count in dictionary_train_en.items()}\n",
    "print(\"Number of unique words in train dataset (only english) after lemmatizing:                              \", len(dictionary_train_en))\n",
    "dictionary_train_en = {word: count for word, count in dictionary_train_en.items() if not word.isnumeric() and word.isalpha()}\n",
    "print(\"Number of unique words in train dataset (only english) after removing numbers and non-alphabetic words:\", len(dictionary_train_en))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
