{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' from '/Users/alfiovavassori/Library/Mobile Documents/com~apple~CloudDocs/Documents/USI/Master/3rd Semester/Advanced Topics in Machine Learning/Project 2/ATML_2/.venv/lib/python3.13/site-packages/pandas/__init__.py' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     AutoTokenizer,\n\u001b[1;32m      8\u001b[0m     AutoModelForMaskedLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     set_seed,\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/USI/Master/3rd Semester/Advanced Topics in Machine Learning/Project 2/ATML_2/.venv/lib/python3.13/site-packages/pandas/__init__.py:139\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    122\u001b[0m     concat,\n\u001b[1;32m    123\u001b[0m     lreshape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     qcut,\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     ExcelFile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     read_spss,\n\u001b[1;32m    173\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/USI/Master/3rd Semester/Advanced Topics in Machine Learning/Project 2/ATML_2/.venv/lib/python3.13/site-packages/pandas/testing.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mPublic testing utility functions.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     assert_extension_array_equal,\n\u001b[1;32m      8\u001b[0m     assert_frame_equal,\n\u001b[1;32m      9\u001b[0m     assert_index_equal,\n\u001b[1;32m     10\u001b[0m     assert_series_equal,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_extension_array_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_frame_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_series_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_index_equal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/USI/Master/3rd Semester/Advanced Topics in Machine Learning/Project 2/ATML_2/.venv/lib/python3.13/site-packages/pandas/_testing/__init__.py:405\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytest\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pytest\u001b[38;5;241m.\u001b[39mraises(expected_exception, match\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 405\u001b[0m cython_table \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241m.\u001b[39mcommon\u001b[38;5;241m.\u001b[39m_cython_table\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[1;32m    409\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    keys and expected result.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' from '/Users/alfiovavassori/Library/Mobile Documents/com~apple~CloudDocs/Documents/USI/Master/3rd Semester/Advanced Topics in Machine Learning/Project 2/ATML_2/.venv/lib/python3.13/site-packages/pandas/__init__.py' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    'train': 'openassistant_best_replies_train.jsonl',\n",
    "    'test':  'openassistant_best_replies_eval.jsonl'\n",
    "}\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "output_dir = \"./Models/Bert/v3/bert-finetuned-mlm\"\n",
    "\n",
    "default_num_train_epochs = 3\n",
    "default_train_batch_size = 8\n",
    "default_eval_batch_size = 8\n",
    "default_learning_rate = 5e-5\n",
    "default_weight_decay = 0.01\n",
    "seed = 42\n",
    "\n",
    "do_hpo = True  \n",
    "\n",
    "n_hpo_trials = 10\n",
    "\n",
    "def hp_space_optuna(trial):\n",
    "    \"\"\"\n",
    "    Optuna parameter search space.#\n",
    "    Adjust to suit your search needs.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"seed\": trial.suggest_categorical(\"seed\", [42, 1234, 2021]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [8, 16, 32]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter search.\n",
    "    We want to minimize the validation loss. The Trainer by default returns 'eval_loss'.\n",
    "    \"\"\"\n",
    "    return metrics[\"eval_loss\"]\n",
    "\n",
    "def main():\n",
    "    set_seed(seed)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"Loading dataset from Hugging Face...\")\n",
    "    \n",
    "    train_data = pd.read_json(f\"hf://datasets/timdettmers/openassistant-guanaco/{splits['train']}\", lines=True)\n",
    "    test_data = pd.read_json(f\"hf://datasets/timdettmers/openassistant-guanaco/{splits['test']}\", lines=True)\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\"text\": train_data[\"text\"].tolist()})\n",
    "    test_dataset = Dataset.from_dict({\"text\": test_data[\"text\"].tolist()})\n",
    "\n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of test samples:     {len(test_dataset)}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    train_tokenized = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    test_tokenized = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "    )\n",
    "\n",
    "    def model_init():\n",
    "        return AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=default_num_train_epochs,\n",
    "        per_device_train_batch_size=default_train_batch_size,\n",
    "        per_device_eval_batch_size=default_eval_batch_size,\n",
    "        learning_rate=default_learning_rate,\n",
    "        weight_decay=default_weight_decay,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init if do_hpo else None,  \n",
    "        model=None if do_hpo else AutoModelForMaskedLM.from_pretrained(model_checkpoint),\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=test_tokenized,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    if do_hpo:\n",
    "        print(\"\\n=== Running hyperparameter search ===\")\n",
    "        best_run = trainer.hyperparameter_search(\n",
    "            direction=\"minimize\",\n",
    "            hp_space=hp_space_optuna,\n",
    "            compute_objective=compute_objective,\n",
    "            n_trials=n_hpo_trials\n",
    "        )\n",
    "        print(\"Best hyperparameters found:\", best_run.hyperparameters)\n",
    "\n",
    "        for k, v in best_run.hyperparameters.items():\n",
    "            setattr(trainer.args, k, v)\n",
    "\n",
    "        trainer.model = trainer.model_init()\n",
    "    else:\n",
    "        print(\"Skipping hyperparameter search...\")\n",
    "\n",
    "    print(\"\\n=== Starting training ===\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\n=== Evaluating on test dataset ===\")\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(f\"Eval metrics: {eval_metrics}\")\n",
    "    if \"eval_loss\" in eval_metrics:\n",
    "        perplexity = torch.exp(torch.tensor(eval_metrics[\"eval_loss\"]))\n",
    "        print(f\"Perplexity: {perplexity.item():.2f}\")\n",
    "\n",
    "    print(f\"\\n=== Saving final model to {output_dir} ===\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(\"All done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    main()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    time_taken = end_time - start_time\n",
    "    print(f\"Time taken to execute the loop: {time_taken:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
