{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning test with masked labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:38:16.926783Z",
     "iopub.status.busy": "2025-01-08T07:38:16.926426Z",
     "iopub.status.idle": "2025-01-08T07:38:37.418922Z",
     "shell.execute_reply": "2025-01-08T07:38:37.417972Z",
     "shell.execute_reply.started": "2025-01-08T07:38:16.926752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install lightning\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:38:53.019794Z",
     "iopub.status.busy": "2025-01-08T07:38:53.019412Z",
     "iopub.status.idle": "2025-01-08T07:39:03.116680Z",
     "shell.execute_reply": "2025-01-08T07:39:03.115731Z",
     "shell.execute_reply.started": "2025-01-08T07:38:53.019751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import lightning as L\n",
    "import gc\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:39:03.118230Z",
     "iopub.status.busy": "2025-01-08T07:39:03.117910Z",
     "iopub.status.idle": "2025-01-08T07:39:03.122141Z",
     "shell.execute_reply": "2025-01-08T07:39:03.121331Z",
     "shell.execute_reply.started": "2025-01-08T07:39:03.118198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SPECIAL_TOKEN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:39:03.123484Z",
     "iopub.status.busy": "2025-01-08T07:39:03.123269Z",
     "iopub.status.idle": "2025-01-08T07:39:04.601489Z",
     "shell.execute_reply": "2025-01-08T07:39:04.600711Z",
     "shell.execute_reply.started": "2025-01-08T07:39:03.123463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text originale:\n",
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n",
    "first_row = df_train.iloc[0]  \n",
    "print(\"Text originale:\")\n",
    "print(first_row[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:39:07.662743Z",
     "iopub.status.busy": "2025-01-08T07:39:07.662256Z",
     "iopub.status.idle": "2025-01-08T07:40:26.450514Z",
     "shell.execute_reply": "2025-01-08T07:40:26.449532Z",
     "shell.execute_reply.started": "2025-01-08T07:39:07.662702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3538\n",
      "191\n",
      "text    ### Human: Can you write a short introduction ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def label_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"error\"\n",
    "    \n",
    "df_train[\"lang\"] = df_train[\"text\"].apply(label_language)\n",
    "df_test[\"lang\"] = df_test[\"text\"].apply(label_language)\n",
    "\n",
    "df_train = df_train[df_train[\"lang\"] == \"en\"]\n",
    "df_test = df_test[df_test[\"lang\"] == \"en\"]\n",
    "df_train = df_train.drop(columns=[\"lang\"])\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(df_train.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:40:26.452014Z",
     "iopub.status.busy": "2025-01-08T07:40:26.451713Z",
     "iopub.status.idle": "2025-01-08T07:40:26.500693Z",
     "shell.execute_reply": "2025-01-08T07:40:26.499676Z",
     "shell.execute_reply.started": "2025-01-08T07:40:26.451977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "print(\":)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:41:53.025544Z",
     "iopub.status.busy": "2025-01-08T07:41:53.025176Z",
     "iopub.status.idle": "2025-01-08T07:41:54.105923Z",
     "shell.execute_reply": "2025-01-08T07:41:54.105179Z",
     "shell.execute_reply.started": "2025-01-08T07:41:53.025501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ef463080ae451aac54944a6e199cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53711fa431074da7b36bc2334b407fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5100edd2b5ff41bf9a9c628524f3b74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efa274bc12d4315b6ae66654f92b446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b8a8202e784be7bc2610bf5e8d2711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import re\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if SPECIAL_TOKEN:\n",
    "    special_tokens_dict = {\"additional_special_tokens\": [\"### Assistant: \", \"### Human: \"]}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "MAX_LEN = 300\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        [sample[\"text\"] for sample in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels_mask = torch.full_like(labels, -100)\n",
    "    for i, sample in enumerate(batch):\n",
    "        text = sample[\"text\"]\n",
    "        for match in re.finditer(r\"### Assistant:\", text):\n",
    "            assistant_start = match.end()  \n",
    "            \n",
    "            next_human_start = text.find(\"### Human:\", assistant_start)\n",
    "            if next_human_start == -1:\n",
    "                next_human_start = len(text)\n",
    "\n",
    "            start_tokens = len(\n",
    "                tokenizer(\n",
    "                    text[:assistant_start],\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    add_special_tokens=False\n",
    "                )[\"input_ids\"]\n",
    "            )\n",
    "\n",
    "            end_tokens = len(\n",
    "                tokenizer(\n",
    "                    text[:next_human_start],\n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LEN,\n",
    "                    add_special_tokens=False\n",
    "                )[\"input_ids\"]\n",
    "            )\n",
    "\n",
    "            labels_mask[i, start_tokens:end_tokens] = labels[i, start_tokens:end_tokens]\n",
    "\n",
    "    labels = labels_mask\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:42:04.454666Z",
     "iopub.status.busy": "2025-01-08T07:42:04.454344Z",
     "iopub.status.idle": "2025-01-08T07:42:11.709876Z",
     "shell.execute_reply": "2025-01-08T07:42:11.709067Z",
     "shell.execute_reply.started": "2025-01-08T07:42:04.454640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## INPUT ########\n",
      "{'input_ids': tensor([[  835, 12968, 29901,  1724,   338,   319, 29902, 29973,  2277, 29937,\n",
      "          4007, 22137, 29901,   306, 29915, 29885,   385,   564,   928,   616,\n",
      "         21082,  2277, 29937, 12968, 29901,  4683,   366,  1854, 29973,  2277,\n",
      "         29937,  4007, 22137, 29901,  3869]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "### Human: What is AI?### Assistant: I'm an arificial intelligence### Human: Are you sure?### Assistant: Yes\n",
      "######## LABELS ########\n",
      "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,   306, 29915, 29885,   385,   564,   928,   616,\n",
      "         21082,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  3869]], device='cuda:0')\n",
      "######## DECODED LABELS ########\n",
      "I'm an arificial intelligence Yes\n"
     ]
    }
   ],
   "source": [
    "text_test = (\n",
    "    \"### Human: What is AI?### Assistant: I'm an arificial intelligence\"\n",
    "    \"### Human: Are you sure?### Assistant: Yes\"\n",
    ")#text_test = '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.'\n",
    "inputs, labels = collate_fn([{\"text\": text_test}])\n",
    "print(\"######## INPUT ########\")\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "print(\"######## LABELS ########\")\n",
    "print(labels)\n",
    "\n",
    "def decode_labels(token_ids, tokenizer):\n",
    "    valid_tokens = [tid for tid in token_ids if tid != -100]\n",
    "    if len(valid_tokens) == 0:\n",
    "        return \"\"\n",
    "    return tokenizer.decode(valid_tokens, skip_special_tokens=False)\n",
    "\n",
    "decoded = decode_labels(labels[0].tolist(), tokenizer)\n",
    "\n",
    "print(\"######## DECODED LABELS ########\")\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:42:25.120959Z",
     "iopub.status.busy": "2025-01-08T07:42:25.120271Z",
     "iopub.status.idle": "2025-01-08T08:01:05.482733Z",
     "shell.execute_reply": "2025-01-08T08:01:05.482034Z",
     "shell.execute_reply.started": "2025-01-08T07:42:25.120930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b365f5e054544c56b8573d7cf57c0ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7122e90c374ffb9170aca622cd4e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e614cf37a4244aed829cc4fabc5d6603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 616 M  | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "615 M     Non-trainable params\n",
      "616 M     Total params\n",
      "2,466.947 Total estimated model params size (MB)\n",
      "442       Modules in train mode\n",
      "315       Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf70bec69a241f6a7c8041c0b1267b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_en_model/tokenizer_config.json',\n",
       " './finetuned_en_model/special_tokens_map.json',\n",
       " './finetuned_en_model/tokenizer.model',\n",
       " './finetuned_en_model/added_tokens.json',\n",
       " './finetuned_en_model/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ####################################\n",
    "# STEP 2 Quantization Configuration\n",
    "# And Model and Tokenizer Loading\n",
    "# ####################################\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3  LoRa\n",
    "# ####################################\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ####################################\n",
    "# STEP 3  Dataset\n",
    "# ####################################\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# ####################################\n",
    "# STEP 5 Lightning Wrapper\n",
    "# ####################################\n",
    "\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, tokenizer, lr=3e-5):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer  # Salva il tokenizer come attributo\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model, tokenizer)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Trainer + Train\n",
    "# ####################################\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints\",\n",
    "    filename=\"finetuned_model-{epoch:02d}-{train_loss:.2f}\",\n",
    "    save_top_k=-1,\n",
    "    save_last=True,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=16,\n",
    "    precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Save the Fine-tuned Model\n",
    "# ####################################\n",
    "\n",
    "model.save_pretrained(\"./finetuned_en_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_en_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Python\n",
      "2. Rust\n",
      "3. Java\n",
      "4. C++\n",
      "5. JavaScript\n",
      "6. Scala\n",
      "7. Go\n",
      "8. AgileScript\n",
      "9. Haxe\n",
      "10. D\n",
      "\n",
      "These are some of the most popular languages used for Artificial Intelligence, but it is important to note that the choice of which language to use will depend on the specific needs and goals of your project. It's always a good idea to research and evaluate the different options before making a decision, to ensure that you choose the language that best suits your needs.### human: What are the pros and cons of each of these languages?Contrary to popular belief, there is no single \"best\" programming language when it comes to artificial intelligence. Each language has its own unique strengths and weaknesses that should be considered when choosing one. Some pros of certain languages include but are not limited to:\n",
      "- High level\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 Load model\n",
    "# ####################################\n",
    "\n",
    "\n",
    "model_path = \"/kaggle/working/finetuned_en_model\"\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "\n",
    "# if False:\n",
    "#     special_tokens_dict = {\"additional_special_tokens\": [\"### Assistant: \", \"### Human: \"]}\n",
    "#     tokenizer.add_special_tokens(special_tokens_dict)\n",
    "#     # Ridimensiona gli embedding del modello base\n",
    "#     base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#         base_model_id,\n",
    "#         torch_dtype=torch.float32,\n",
    "#         device_map={\"\": device} \n",
    "#     )\n",
    "        \n",
    "#     base_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "#     model = PeftModel.from_pretrained(base_model, model_path)\n",
    "#     model = model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=None\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 prepare prompt\n",
    "# ####################################\n",
    "\n",
    "query = \"List the best programming languages for AI\"\n",
    "prompt = f\"### Human: {query} ### Assistant: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 generate output\n",
    "# ####################################\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200, \n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "generated_response = response.split(\"### Assistant: \")[-1].strip()\n",
    "\n",
    "print(generated_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samu/Desktop/Usi/Advanced Topics in Machine Learning/ATML_2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/samu/Desktop/Usi/Advanced Topics in Machine Learning/ATML_2/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "you: hello how are you?\n",
      "Bot: Hello! I'm an AI created by LAION, so I don't experience emotions the way humans do. However, I am functioning properly and ready to assist you in any way I can. How can I help you today? Is there something you want to know? Or do you just have a question?â£ â—¡ï¸Â¿Tienes alguna pregunta o inquietud? Â¿DÃ³nde puedo encontrar respuestas a ella? ou simplemente deseas saber mÃ¡s sobre una de mis funciÃ³nn?\n",
      "you: do you know python?\n",
      "Bot: Python is a popular programming language that is used for a variety of different purposes, including web development, data analysis, machine learning, and more. While it is not specifically mentioned in the question, it's likely that you are referring to Python, the Python interpreter and runtime, which is included with most Python distributions.\n",
      "\n",
      "Python is an interpreted language, meaning that the code you write is executed when you run it, rather than being processed by a processor as opposed to executing code. This makes it very powerful and versatile, as it allows you to write code that can handle a wide range of tasks, from simple data manipulation and analysis to complex web applications and deep learning algorithms. Python also has a large and active community of developers, who contribute to its development and documentation, making it a great choice for both beginners and experienced developers alike. Ultimately, if you're interested in programming, Python should be at the top of your list of options.\n",
      "you: How do I import TensorFlow into Python?\n",
      "Bot: You can easily import the tensorflow library into python using the following code:\n",
      "import tensorflow as tf\n",
      "\n",
      "This will create a new variable named tensorflow with a value of True.\n",
      "Now you can use tensorflow in your python code as usual. \n",
      "Hope this helps! ðŸ˜Š\n",
      "you: I tried `import tensorflow as tf`, but I get the error: `ModuleNotFoundError: No module named 'tensorflow'`. What should I do?\n",
      "Bot: To import the TensorFlow library, you need to specify its path using the `PATH` environment variable. For example, if you are on a Linux system, the command to set the PATH variable is:\n",
      "\n",
      "```\n",
      "export PATH=/usr/local/sbin:$PATH\n",
      "source /etc/profile.d/paths.sh # If you're on Windows, replace '.' with '\\\\.' in the path variable to reflect system-wide paths\n",
      "echo \"$PATH\" | tee -a ${HOME}/.profile # Save the variable in a file with a `.profile` extension (e.g., `.bashrc`) in your user's home directory (usually $HOME)\n",
      "````\n",
      "In case you want to use a virtual environment, it is possible to add the required libraries to the system environment using `source deactivate` before importing the library. The command can be added to a `bash` or `zsh` terminal:\n",
      "you: perfect, everything worked thank you very much. Bye\n",
      "Bot: Glad to hear it! I'm happy to assist you in any way I can. If you have any questions or need additional information, feel free to ask. ðŸ˜Š\n",
      "\n",
      "Regards,\n",
      "[Your Name]\n",
      "you: esc\n",
      "END :)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "MEMORY_SAVING = False\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_path = \"Models/model_finetuned_en\"\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=None\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "prev_prompt = \"\" \n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    print(\"you:\", user_input)\n",
    "    \n",
    "    if user_input.lower() == \"esc\":\n",
    "        print(\"END :)\")\n",
    "        break\n",
    "    \n",
    "    prompt = f\"### Human: {user_input} ### Assistant:\"\n",
    "    prompt = prev_prompt + prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Assistant:\")[-1].strip()\n",
    "    response = response.split(\"###\")[0].strip()\n",
    "    \n",
    "    if MEMORY_SAVING:\n",
    "        prev_prompt += f\"### Human: {user_input} ### Assistant: {response} ###\"\n",
    "    \n",
    "    print(f\"Bot: {response}\")\n",
    "\n",
    "#hello how are you?\n",
    "#do you know python?\n",
    "#How do I import TensorFlow into Python?\n",
    "#I tried `import tensorflow as tf`, but I get the error: `ModuleNotFoundError: No module named 'tensorflow'`. What should I do?\n",
    "#perfect, everything worked thank you very much. Bye"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
