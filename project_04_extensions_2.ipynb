{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install lightning\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import lightning as L\n",
    "import gc\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text originale:\n",
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n",
    "first_row = df_train.iloc[0]  \n",
    "print(\"Text originale:\")\n",
    "print(first_row[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non english text\n",
    "\n",
    "def label_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"error\"\n",
    "    \n",
    "df_train[\"lang\"] = df_train[\"text\"].apply(label_language)\n",
    "df_test[\"lang\"] = df_test[\"text\"].apply(label_language)\n",
    "\n",
    "df_train = df_train[df_train[\"lang\"] == \"en\"]\n",
    "df_test = df_test[df_test[\"lang\"] == \"en\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'lang'\n",
    "df_train = df_train.drop(columns=[\"lang\"])\n",
    "train_dataset = Dataset.from_pandas(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Tokenizza il testo\n",
    "    inputs = tokenizer(\n",
    "        [sample['text'] for sample in batch], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=400\n",
    "    )\n",
    "    # Clona input_ids per usarli come etichette\n",
    "    labels = inputs.input_ids.clone()\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Applica maschera per ignorare la parte di istruzione (### Human: ...) e tener conto solo della risposta\n",
    "    for i, sample in enumerate(batch):\n",
    "        text = sample['text']\n",
    "        # Trova l'indice di inizio della risposta\n",
    "        response_start = text.find(\"### Assistant:\")\n",
    "        if response_start != -1:\n",
    "            # Calcola la lunghezza in token fino alla risposta\n",
    "            response_start_token_idx = tokenizer(\n",
    "                text[:response_start], \n",
    "                truncation=True, \n",
    "                max_length=450, \n",
    "                return_tensors=\"pt\"\n",
    "            )[\"input_ids\"].size(1)\n",
    "            # Maschera tutto ci√≤ che precede la risposta\n",
    "            labels[i, :response_start_token_idx] = pad_token_id\n",
    "    \n",
    "    # Trasferisci i tensori sul dispositivo\n",
    "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "    labels = labels.to(device)\n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## INPUT ########\n",
      "{'input_ids': tensor([[    1,   835, 12968, 29901,  1724,   338,   319, 29902, 29973,   835,\n",
      "          4007, 22137, 29901,   319, 29875,   338,   592]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<s> ### Human: What is AI? ### Assistant: Ai is me\n",
      "######## LABELS ########\n",
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "          4007, 22137, 29901,   319, 29875,   338,   592]])\n",
      "######## DECODED LABELS ########\n",
      "Assistant: Ai is me\n"
     ]
    }
   ],
   "source": [
    "text_test = \"### Human: What is AI? ### Assistant: Ai is me\"\n",
    "inputs, labels = collate_fn([{\"text\": text_test}])\n",
    "print(\"######## INPUT ########\")\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "print(\"######## LABELS ########\")\n",
    "print(labels)\n",
    "decoded_labels = tokenizer.decode(\n",
    "    [token for token in labels[0].tolist() if token != tokenizer.pad_token_id]\n",
    ")\n",
    "print(\"######## DECODED LABELS ########\")\n",
    "print(decoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.init(\n",
    "    project=\"anime_Lama\", \n",
    "    name=\"anime_lama_1\",\n",
    "    config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 Quantization Configuration\n",
    "# And Model and Tokenizer Loading\n",
    "# ####################################\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3  LoRa\n",
    "# ####################################\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ####################################\n",
    "# STEP 3  Dataser\n",
    "# ####################################\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    batch_size=10,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 5 Lightning Wrapper\n",
    "# ####################################\n",
    "\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # Shift logits and labels\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        # Compute LM loss\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # Log loss to wandb\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Trainer + Train\n",
    "# ####################################\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints\",\n",
    "    filename=\"finetuned_model-{epoch:02d}-{train_loss:.2f}\",\n",
    "    save_top_k=-1,\n",
    "    save_last=True,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"finetuned_en_model\", \n",
    "    log_model=True \n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,  # Aggiungi il logger\n",
    "    accumulate_grad_batches=8,\n",
    "    precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Save the Fine-tuned Model\n",
    "# ####################################\n",
    "\n",
    "model.save_pretrained(\"./finetuned_en_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_en_model\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
