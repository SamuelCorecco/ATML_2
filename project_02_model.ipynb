{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T11:08:21.195046Z",
     "iopub.status.busy": "2024-12-17T11:08:21.194436Z",
     "iopub.status.idle": "2024-12-17T11:08:21.198820Z",
     "shell.execute_reply": "2024-12-17T11:08:21.197962Z",
     "shell.execute_reply.started": "2024-12-17T11:08:21.195011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T11:08:22.662382Z",
     "iopub.status.busy": "2024-12-17T11:08:22.662049Z",
     "iopub.status.idle": "2024-12-17T11:09:01.826559Z",
     "shell.execute_reply": "2024-12-17T11:09:01.825288Z",
     "shell.execute_reply.started": "2024-12-17T11:08:22.662353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install rouge-score\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install bitsandbytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T11:09:01.828789Z",
     "iopub.status.busy": "2024-12-17T11:09:01.828483Z",
     "iopub.status.idle": "2024-12-17T11:09:16.562536Z",
     "shell.execute_reply": "2024-12-17T11:09:16.561703Z",
     "shell.execute_reply.started": "2024-12-17T11:09:01.828761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import wandb\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n",
    "first_row = df_train.iloc[0]  \n",
    "print(\"Text originale:\")\n",
    "print(first_row[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Evaluation\n",
    "the first thing to do is to understand how our practice model is set up, following what they did on huggingface the model uses https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.1 as a base, so before finetuning we want to understand how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T19:24:13.695149Z",
     "iopub.status.busy": "2024-12-16T19:24:13.694222Z",
     "iopub.status.idle": "2024-12-16T19:24:16.715588Z",
     "shell.execute_reply": "2024-12-16T19:24:16.714701Z",
     "shell.execute_reply.started": "2024-12-16T19:24:13.695094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "### Human: What is the best programming language for Machine Learning? ### Assistant: There are many different types of machine learning algorithms, and there isn't one definitive \"best\" choice. It depends on your specific needs as an A\n"
     ]
    }
   ],
   "source": [
    "model = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0 if device == \"cuda\" else -1,  # GPU: device=0, CPU: device=-1\n",
    ")\n",
    "\n",
    "prompt = \"What is the best programming language for Machine Learning?\"\n",
    "formatted_prompt = f\"### Human: {prompt} ### Assistant:\"\n",
    "sequences = pipeline(\n",
    "    formatted_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.5,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=32,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(seq[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:38:38.047892Z",
     "iopub.status.busy": "2024-12-16T18:38:38.047153Z",
     "iopub.status.idle": "2024-12-16T18:40:01.612057Z",
     "shell.execute_reply": "2024-12-16T18:40:01.611183Z",
     "shell.execute_reply.started": "2024-12-16T18:38:38.047856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce0ea90cab542a49c33322ed5aa8ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss for the original model on the test set: 6.530913352966309\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# ####################################\n",
    "# STEP 1: Load model and tokenizer\n",
    "# ####################################\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as the padding token\n",
    "\n",
    "# ####################################\n",
    "# STEP 2: Prepare data in correct format\n",
    "# ####################################\n",
    "def format_data(df):\n",
    "    df[\"text\"] = df.apply(\n",
    "        lambda x: f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n{x['reference']}<|im_end|>\\n\", \n",
    "        axis=1\n",
    "    )\n",
    "    return Dataset.from_pandas(df[[\"text\"]])\n",
    "\n",
    "test_dataset = format_data(df_test)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# ####################################\n",
    "# STEP 3: Trainer for evaluate\n",
    "# ####################################\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_original_model\",\n",
    "    per_device_eval_batch_size=8, \n",
    "    fp16=True, \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_test_dataset, \n",
    ")\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 4: Evaluate and print the test loss\n",
    "# ####################################\n",
    "results = trainer.evaluate()\n",
    "print(f\"Mean Loss for the original model on the test set: {results['eval_loss']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First \"big\" fine-tuned model\n",
    "\n",
    "In this section we do a finetuning using Transformer Trainer. This finetuning trains the entire model. It is a full finetuning approach\n",
    "\n",
    "### 1 The data is formatted for the chat task:\n",
    "First the data must be prepared in a precise format, at the moment we have decided, as for the other approaches to have a format delimited by <|im_start|> <|im_end|>.\n",
    "\n",
    "###  2 A pre-trained model is loaded \n",
    "We then load the model with a reduced accuracy (FP16) as we have neither the resources nor the time to be able to use better accuracies. We tokenise train and test set.\n",
    "\n",
    "###  3 Transformer Trainer is applied for fine-tuning\n",
    "LoRa is configured for causal language modelling (CAUSAL_LM), with lora we add adapters to the model. In this way we can train few parameters compared to the total.\n",
    "\n",
    "###  4 We configure the trainer and start the training.\n",
    "We then configure the trainer, using 1 batch_size, the accumulated gradient of 8. In this way we can simulate a batch_size of 8 (without making the training too heavy). This is because more and the programme crashes due to too much GPU usage. We also use the precision bfloat16 so as not to make the training too heavy\n",
    "\n",
    "### 5 We save the results and monitor the progress with W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:18:39.626373Z",
     "iopub.status.busy": "2024-12-16T16:18:39.625668Z",
     "iopub.status.idle": "2024-12-16T18:20:47.547966Z",
     "shell.execute_reply": "2024-12-16T18:20:47.546944Z",
     "shell.execute_reply.started": "2024-12-16T16:18:39.626325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ikgyhaxu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_1</strong> at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/ikgyhaxu' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/ikgyhaxu</a><br/> View project at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241216_161813-ikgyhaxu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ikgyhaxu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241216_161839-n0gvn5d8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/n0gvn5d8' target=\"_blank\">experiment_1</a></strong> to <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/n0gvn5d8' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/n0gvn5d8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f007a47af134d9f95c47c5c255882f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c7b4e4cb114d80a50b13b9167468a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1230/1230 2:01:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>1.178330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completato e modello salvato!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"tiny-llama-finetuning\", name=\"experiment_1\")\n",
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 we make data in correct format\n",
    "# STEP 2 We load the model\n",
    "# ####################################\n",
    "\n",
    "def extract_prompt_and_reference(row):\n",
    "    parts = row.split(\"### Assistant:\")\n",
    "    prompt = parts[0].strip()\n",
    "    reference = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    return prompt, reference\n",
    "\n",
    "df_train[[\"prompt\", \"reference\"]] = df_train[\"text\"].apply(lambda x: pd.Series(extract_prompt_and_reference(x)))\n",
    "df_test[[\"prompt\", \"reference\"]] = df_test[\"text\"].apply(lambda x: pd.Series(extract_prompt_and_reference(x)))\n",
    "\n",
    "def format_data(df):\n",
    "    df[\"text\"] = df.apply(\n",
    "        lambda x: f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n{x['reference']}<|im_end|>\", \n",
    "        axis=1\n",
    "    )\n",
    "    return Dataset.from_pandas(df[[\"text\"]])\n",
    "\n",
    "train_dataset = format_data(df_train)\n",
    "test_dataset = format_data(df_test)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PY007/TinyLlama-1.1B-Chat-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"PY007/TinyLlama-1.1B-Chat-v0.1\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    config={\"dropout\": 0.1}\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 Train + Trainer\n",
    "# ####################################\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"tiny-llama-run\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 4 Save\n",
    "# ####################################\n",
    "\n",
    "AttributeError.save_model(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:23:38.490076Z",
     "iopub.status.busy": "2024-12-16T18:23:38.489701Z",
     "iopub.status.idle": "2024-12-16T18:23:42.710963Z",
     "shell.execute_reply": "2024-12-16T18:23:42.710110Z",
     "shell.execute_reply.started": "2024-12-16T18:23:38.490042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "What is the best programming language for Machine Learning?\n",
      "\n",
      "Risposta Generata:\n",
      "<|im_start|>user\n",
      "What is the best programming language for Machine Learning?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The best programming language for machine learning largely depends on your specific needs and goals. However, popular options include Python, R, Java, and Scala. Each has its own strengths and weaknesses in terms of ease of use, performance, and scalability. It's important to research and compare different programming languages to determine which one is best suited for your specific needs.### Human: What are some of the most popular Machine Learning frameworks available today?<|im_end|\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ####################################\n",
    "# STEP 1: Load model\n",
    "# ####################################\n",
    "\n",
    "model_path = \"./finetuned_model\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# ####################################\n",
    "# STEP 2: Generate reosponse\n",
    "# ####################################\n",
    "\n",
    "prompt = \"What is the best programming language for Machine Learning?\"\n",
    "formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100, \n",
    "        do_sample=True, \n",
    "        top_k=50, \n",
    "        top_p=0.7,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ####################################\n",
    "# STEP 3: print result\n",
    "# ####################################\n",
    "\n",
    "print(\"\\nPrompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nRisposta Generata:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second model vertion of fine-tuning\n",
    "\n",
    "In this section we do a finetuning using the Low-Rank Adaptation LoRA approach. <br>\n",
    "### 1 Data is formatted for the chat task:\n",
    "First the data must be prepared in a precise format, at the moment we have decided, as for the other approaches, to have a format delimited by <|im_start|> <|im_end|>.\n",
    "### 2 Load pre-trained\n",
    "Then the model is loaded with reduced precision (FP16), as we have neither the resources nor the time to be able to use better accuracies.\n",
    "\n",
    "### 3 LoRA\n",
    "LoRa is configured for causal language modelling (CAUSAL_LM), with which we add adapters to the model. In this way we can train few parameters compared to the total.\n",
    "\n",
    "### 4 Configuration for trainer + Train\n",
    "Next we configured the trainer, using 8 batch_sizes, the gradient accumulated by 4. In this way we can simulate a batch_size of 32 (without making the training too heavy). As another speedup we also have FP16 precision.\n",
    "\n",
    "### 5 Save model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T10:55:27.517130Z",
     "iopub.status.busy": "2024-12-16T10:55:27.516270Z",
     "iopub.status.idle": "2024-12-16T15:27:32.136635Z",
     "shell.execute_reply": "2024-12-16T15:27:32.135736Z",
     "shell.execute_reply.started": "2024-12-16T10:55:27.517093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6hojlicl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_lora</strong> at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/6hojlicl' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/6hojlicl</a><br/> View project at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241216_105430-6hojlicl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6hojlicl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241216_105527-lise0oh1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1' target=\"_blank\">experiment_lora</a></strong> to <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4e982b6e4e44cab68e000464f446a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 4:31:38, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.747800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.727200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.657600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.702300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.690600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.718300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.667300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.670100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.674900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.631300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.670900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.654200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.726500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.652900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.665600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.671400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.662300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.678500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.702200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello fine-tunato salvato in ./finetuned_tinyllama_lora\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.4935145454855782e+17</td></tr><tr><td>train/epoch</td><td>4.87734</td></tr><tr><td>train/global_step</td><td>1500</td></tr><tr><td>train/grad_norm</td><td>0.86986</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6551</td></tr><tr><td>train_loss</td><td>0.68433</td></tr><tr><td>train_runtime</td><td>16308.9503</td></tr><tr><td>train_samples_per_second</td><td>2.943</td></tr><tr><td>train_steps_per_second</td><td>0.092</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_lora</strong> at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1</a><br/> View project at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241216_105527-lise0oh1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(project=\"tiny-llama-finetuning\", name=\"experiment_lora\")\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "output_dir = \"./finetuned_tinyllama_lora\"\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 we make data in correct format\n",
    "# ####################################\n",
    "\n",
    "def format_data(df):\n",
    "    df[\"text\"] = df.apply(\n",
    "        lambda x: f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n{x['reference']}<|im_end|>\\n\", \n",
    "        axis=1\n",
    "    )\n",
    "    return Dataset.from_pandas(df[[\"text\"]])\n",
    "train_dataset = format_data(df_train)\n",
    "test_dataset = format_data(df_test)\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 We load the model\n",
    "# ####################################\n",
    "    \n",
    "def get_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float16 \n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(base_model_id)\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [\"<|im_start|>\", \"<|im_end|>\"]})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 LoRA\n",
    "# ####################################\n",
    "peft_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 4 Training configuration + Trainer\n",
    "# ####################################\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"tiny-llama-run\",\n",
    "    max_steps=1500\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    max_seq_length=512, \n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 5 Save the model\n",
    "# ####################################\n",
    "\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "trainer.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of our second model (fine-tuning with LoRA)\n",
    "\n",
    "In this case, we do two things, the first is to assess whether the model works correctly, i.e. whether it correctly generates the response\n",
    "<br><br>\n",
    "Next, we test the loss on the test set to see how the model performs and validate whether it has actually learnt in the training.<br>\n",
    "For this part we use the trainer, which is a convenient and similar implementation to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:48:37.914542Z",
     "iopub.status.busy": "2024-12-16T15:48:37.913707Z",
     "iopub.status.idle": "2024-12-16T15:48:43.875303Z",
     "shell.execute_reply": "2024-12-16T15:48:43.874450Z",
     "shell.execute_reply.started": "2024-12-16T15:48:37.914506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello di base...\n",
      "Caricamento degli adattatori LoRA...\n",
      "Caricamento del tokenizer...\n",
      "Esempio di generazione del testo...\n",
      "\n",
      "Risposta Generata:\n",
      "<|im_start|>user\n",
      "What is the best programming language for Machine Learning?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "There are many programming languages used for machine learning, but the most common and popular choice is Python. Python is a high-level, versatile language that is known for its ease of use and widespread adoption in the AI industry. It has a large and active community of developers who develop and release new libraries and modules to further enhance the language's capabilities.\n",
      "\n",
      "Other popular choices for machine learning programming languages include R, Java, C++, C#, and Scala.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 Load model + LoRa + tokenizer\n",
    "# ####################################\n",
    "\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "adapter_path = \"./finetuned_tinyllama_lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Imposta il token di padding come EOS\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 Generate a response in our format\n",
    "# ####################################\n",
    "\n",
    "prompt = \"What is the best programming language for Machine Learning?\"\n",
    "formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.7,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated response:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:54:25.666383Z",
     "iopub.status.busy": "2024-12-16T15:54:25.665948Z",
     "iopub.status.idle": "2024-12-16T15:55:48.344881Z",
     "shell.execute_reply": "2024-12-16T15:55:48.344013Z",
     "shell.execute_reply.started": "2024-12-16T15:54:25.666350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello di base...\n",
      "Caricamento degli adattatori LoRA...\n",
      "Caricamento del tokenizer...\n",
      "Configurazione del Trainer per la valutazione...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52e6260c801446b9166e9cf39d3698b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo della loss sul test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss media sul test set: 1.8103272914886475\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 Load model + LoRa + tokenizer\n",
    "# ####################################\n",
    "\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "adapter_path = \"./finetuned_tinyllama_lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 make test in correct form\n",
    "# ####################################\n",
    "\n",
    "def format_data(df):\n",
    "    df[\"text\"] = df.apply(\n",
    "        lambda x: f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n{x['reference']}<|im_end|>\\n\", \n",
    "        axis=1\n",
    "    )\n",
    "    return Dataset.from_pandas(df[[\"text\"]])\n",
    "\n",
    "test_dataset = format_data(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 Trainer using for evaluation\n",
    "# ####################################\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=True, \n",
    "    report_to=\"none\", \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 4 get the evaluation of our model\n",
    "# ####################################\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"mean Loss on our test set: {results['eval_loss']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Approach QLoRA Fine-Tuning Approach\n",
    "We wanted to follow a bit of the style seen in class, with the finetuning shown in the tutorial\n",
    "<br>\n",
    "fine-tuning using Quantized Low-Rank Adaptation (QLoRA). This approach is efficient and reduces memory consumption by combining 4-bit quantization with LoRA adapters. It allows us to fine-tune large language models even on hardware with limited resources.\n",
    "\n",
    "### 1. Data Formatting\n",
    "First, the dataset is preprocessed to match the Alpaca-style instruction-response format. \n",
    "\n",
    "### 2. Model Loading with 4-bit Quantization\n",
    "The pre-trained base model is loaded using 4-bit quantization. The quantization type is NF4, precision BF16 \n",
    "\n",
    "### 3. LoRA Configuration\n",
    "\n",
    "### 4. Trainer Configuration and Training\n",
    "Since it was more efficient we could use a larger batch size, we used 2\n",
    "\n",
    "### 5. Model Saving\n",
    "\n",
    "### NOTE\n",
    "we had to ‘kill’ the kernel, since in kaggle or Colab to use quantisation after installing packages the kernel has to be restarted. From VS code you can also restart by hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T11:39:37.417729Z",
     "iopub.status.busy": "2024-12-17T11:39:37.417348Z",
     "iopub.status.idle": "2024-12-17T11:39:57.395319Z",
     "shell.execute_reply": "2024-12-17T11:39:57.394168Z",
     "shell.execute_reply.started": "2024-12-17T11:39:37.417696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install lightning\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:01:27.286745Z",
     "iopub.status.busy": "2024-12-17T12:01:27.285746Z",
     "iopub.status.idle": "2024-12-17T12:02:48.182699Z",
     "shell.execute_reply": "2024-12-17T12:02:48.181958Z",
     "shell.execute_reply.started": "2024-12-17T12:01:27.286705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 we make data in correct format\n",
    "# ####################################\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', \n",
    "          'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n",
    "def filter_english(df):\n",
    "    filtered_texts = []\n",
    "    for text in df['text']:\n",
    "        try:\n",
    "            if detect(text) == \"en\":\n",
    "                filtered_texts.append(text)\n",
    "        except LangDetectException:\n",
    "            continue\n",
    "    return filtered_texts\n",
    "\n",
    "train_texts = filter_english(df_train)\n",
    "test_texts = filter_english(df_test)\n",
    "\n",
    "train_texts = train_texts[:int(len(train_texts) * 1.0)]\n",
    "test_texts = test_texts[:int(len(test_texts) * 1.0)]\n",
    "\n",
    "# 3. Alpaca Format\n",
    "\n",
    "def format_prompts(texts):\n",
    "    formatted_texts = []\n",
    "    for text in texts:\n",
    "        if \"### Human:\" in text and \"### Assistant:\" in text:\n",
    "            parts = text.split(\"### Human:\")\n",
    "            for part in parts[1:]:\n",
    "                try:\n",
    "                    human, assistant = part.split(\"### Assistant:\", 1)\n",
    "                    formatted_text = alpaca_prompt.format(human.strip(), \"\", assistant.strip())\n",
    "                    formatted_texts.append(formatted_text)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return formatted_texts\n",
    "\n",
    "train_formatted = format_prompts(train_texts)\n",
    "test_formatted = format_prompts(test_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:02:48.190526Z",
     "iopub.status.busy": "2024-12-17T12:02:48.190281Z",
     "iopub.status.idle": "2024-12-17T12:02:48.202371Z",
     "shell.execute_reply": "2024-12-17T12:02:48.201647Z",
     "shell.execute_reply.started": "2024-12-17T12:02:48.190502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is an AI?\\n\\n### Input:\\n\\n\\n### Response:\\nAI is me']\n"
     ]
    }
   ],
   "source": [
    "text = [\"### Human: What is an AI? ### Assistant: AI is me\"]\n",
    "print(format_prompts(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:05:43.884621Z",
     "iopub.status.busy": "2024-12-17T12:05:43.884272Z",
     "iopub.status.idle": "2024-12-17T12:44:06.513281Z",
     "shell.execute_reply": "2024-12-17T12:44:06.512279Z",
     "shell.execute_reply.started": "2024-12-17T12:05:43.884591Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 616 M  | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "615 M     Non-trainable params\n",
      "616 M     Total params\n",
      "2,466.947 Total estimated model params size (MB)\n",
      "442       Modules in train mode\n",
      "315       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e1d83a1fa488bb70c26b2c5da07f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completato e modello salvato!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import lightning as L\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 Load quantizate model\n",
    "# ####################################\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_formatted})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_formatted})\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  \n",
    "    bnb_4bit_quant_type=\"nf4\",              \n",
    "    bnb_4bit_use_double_quant=True          \n",
    ")\n",
    "\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",          \n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 LoRa\n",
    "# ####################################\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model) \n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        [sample['text'] for sample in batch], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    labels = inputs.input_ids.clone()\n",
    "    labels[~inputs.attention_mask.bool()] = -100\n",
    "    return inputs, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    collate_fn=collate_fn, \n",
    "    shuffle=True, \n",
    "    batch_size=2, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# just as we see in class\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "        # Shift logits to exclude the last element\n",
    "        # shift labels to exclude the first element\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        # Compute LM loss token-wise\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 4 Trainer + Train\n",
    "# ####################################\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=64,\n",
    "    precision=\"bf16-mixed\", \n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=1\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 5 Save\n",
    "# ####################################\n",
    "\n",
    "model.save_pretrained(\"./finetuned_qlora_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_qlora_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:53:37.234458Z",
     "iopub.status.busy": "2024-12-17T12:53:37.233395Z",
     "iopub.status.idle": "2024-12-17T12:53:48.623589Z",
     "shell.execute_reply": "2024-12-17T12:53:48.622656Z",
     "shell.execute_reply.started": "2024-12-17T12:53:37.234416Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated Response ###\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the best programming language for Machine Learning?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response: \n",
      "There are many programming languages that can be used for machine learning, but the choice depends on your specific needs and preferences. Some popular options include Python, R, Java, and Scala. Each language has its own strengths and weaknesses, so it is important to do your research and choose the one that best fits your needs. Additionally, you may want to consider the type of data that you have and the level of complexity you are looking to achieve when making your decision.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 Load model\n",
    "# ####################################\n",
    "\n",
    "model_path = \"./finetuned_qlora_model\"\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 prepare prompt\n",
    "# ####################################\n",
    "\n",
    "instruction = \"What is the best programming language for Machine Learning?\"\n",
    "input_context = \"\"\n",
    "prompt_2 = (\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "    f\"### Input:\\n{input_context}\\n\\n\"\n",
    "    \"### Response:\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt_2, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 generate output\n",
    "# ####################################\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100, \n",
    "        num_beams=4,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "generated_response = response.split(\"### Assistant:\")[-1].strip()\n",
    "\n",
    "print(generated_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR MAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samu/Desktop/Usi/Advanced Topics in Machine Learning/ATML_2/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Which is the most famous anime?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response: \n",
      "Based on popularity, popular animes include Sword Art Online, Fullmetal Alchemist, Naruto, and Dragon Ball Z. However, it's difficult to say which one is more famous as there are many different factors that can influence this. Some factors to consider include the quality of the animation, storyline, characters, music, manga and light novels, as well as the impact the series has had on the industry and its influence on pop culture.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 Load model\n",
    "# ####################################\n",
    "\n",
    "# on mac\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model_path = \"Models/finetuned_qlora_model\"\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": device} \n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 prepare prompt\n",
    "# ####################################\n",
    "\n",
    "instruction = \"Which is the most famous anime?\"\n",
    "input_context = \"\"\n",
    "prompt_2 = (\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "    f\"### Input:\\n{input_context}\\n\\n\"\n",
    "    \"### Response:\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt_2, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 generate output\n",
    "# ####################################\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100, \n",
    "        num_beams=4,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "generated_response = response.split(\"### Assistant:\")[-1].strip()\n",
    "\n",
    "print(generated_response)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
