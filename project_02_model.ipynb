{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:10:48.299282Z","iopub.execute_input":"2024-12-16T16:10:48.300235Z","iopub.status.idle":"2024-12-16T16:10:48.322006Z","shell.execute_reply.started":"2024-12-16T16:10:48.300196Z","shell.execute_reply":"2024-12-16T16:10:48.321391Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install rouge-score\n!pip install peft\n!pip install trl\n!pip install bitsandbytes\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:10:49.700388Z","iopub.execute_input":"2024-12-16T16:10:49.700716Z","iopub.status.idle":"2024-12-16T16:11:28.458390Z","shell.execute_reply.started":"2024-12-16T16:10:49.700687Z","shell.execute_reply":"2024-12-16T16:11:28.457409Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import Dataset\nfrom tqdm import tqdm\n\nimport transformers\nimport torch\nimport pandas as pd\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport torch\nimport wandb\nfrom transformers import EarlyStoppingCallback\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:11:28.460619Z","iopub.execute_input":"2024-12-16T16:11:28.461267Z","iopub.status.idle":"2024-12-16T16:11:48.744236Z","shell.execute_reply.started":"2024-12-16T16:11:28.461221Z","shell.execute_reply":"2024-12-16T16:11:48.743328Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nsplits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\ndf_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\ndf_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n\nfirst_row = df_train.iloc[0]  \nprint(\"Text originale:\")\nprint(first_row[\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:19:47.143947Z","iopub.execute_input":"2024-12-16T19:19:47.144590Z","iopub.status.idle":"2024-12-16T19:19:47.794444Z","shell.execute_reply.started":"2024-12-16T19:19:47.144553Z","shell.execute_reply":"2024-12-16T19:19:47.793533Z"}},"outputs":[{"name":"stdout","text":"Text originale:\n### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n\nReferences:\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"# Checkpoint Evaluation\nthe first thing to do is to understand how our practice model is set up, following what they did on huggingface the model uses https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.1 as a base, so before finetuning we want to understand how the model performs","metadata":{}},{"cell_type":"code","source":"model = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device=0 if device == \"cuda\" else -1,  # GPU: device=0, CPU: device=-1\n)\n\nprompt = \"What do you think of Evangelion\"\nformatted_prompt = f\"### Human: {prompt} ### Assistant:\"\nsequences = pipeline(\n    formatted_prompt,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    repetition_penalty=1.5,\n    eos_token_id=tokenizer.eos_token_id,\n    max_new_tokens=32,\n)\nfor seq in sequences:\n    print(seq[\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:57:01.521962Z","iopub.execute_input":"2024-12-16T09:57:01.522592Z","iopub.status.idle":"2024-12-16T09:57:04.112841Z","shell.execute_reply.started":"2024-12-16T09:57:01.522561Z","shell.execute_reply":"2024-12-16T09:57:04.112069Z"}},"outputs":[{"name":"stdout","text":"cuda\n### Human: What do you think of Evangelion ### Assistant: As an AI language model, I don't have feelings or opinions. My job is to provide information and help users ask questions if needed.###\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# ####################################\n# STEP 1: Carica il modello originale e il tokenizer\n# ####################################\n\nmodel_name = \"PY007/TinyLlama-1.1B-Chat-v0.1\"  # Nome del modello originale\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    torch_dtype=torch.float16, \n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token  # Imposta il token di padding come EOS\n\n# ####################################\n# STEP 2: Genera una risposta dal modello originale\n# ####################################\n\n# Prompt da testare\nprompt = \"What is the best programming language for Machine Learning?\n\n# Prepara il prompt nel formato richiesto dal modello\nformatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n\n# Tokenizza il prompt e preparalo per l'inferenza\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# Genera la risposta\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100,  # Numero massimo di token nella risposta\n        do_sample=True,  # Campiona in modo casuale\n        top_k=50,  # Considera i top 50 token per ogni passaggio\n        top_p=0.7,  # Usa nucleus sampling (somma di probabilità fino al 70%)\n        temperature=0.7,  # Controlla la casualità delle risposte\n        repetition_penalty=1.1  # Penalizza ripetizioni\n    )\n\n# Decodifica la risposta generata\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# ####################################\n# STEP 3: Stampa il risultato\n# ####################################\n\nprint(\"\\nPrompt:\")\nprint(prompt)\nprint(\"\\nRisposta Generata:\")\nprint(generated_text)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T18:32:50.717599Z","iopub.execute_input":"2024-12-16T18:32:50.717941Z","iopub.status.idle":"2024-12-16T18:32:56.347625Z","shell.execute_reply.started":"2024-12-16T18:32:50.717914Z","shell.execute_reply":"2024-12-16T18:32:56.346634Z"}},"outputs":[{"name":"stdout","text":"Object `Learning` not found.\n\nPrompt:\nWhat is the best programming language for Machine Learning?\n\nRisposta Generata:\n<|im_start|>user\nWhat is the best programming language for Machine Learning?<|im_end|>\n<|im_start|>assistant\nThe best programming languages for machine learning are Rust, Python, and Java.\nThe advantages of Rust over other languages include its safety-first mentality, efficient memory management, and its ability to compile down to a low-level language that can be easily implemented on top of other systems.\nPython also has strong support for machine learning models, and it's a great choice for building applications that require high performance and scalability. It's also easy to learn and use, making\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom datasets import Dataset\n\n# ####################################\n# STEP 1: Load model and tokenizer\n# ####################################\nbase_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token = tokenizer.eos_token  # Set EOS token as the padding token\n\n# ####################################\n# STEP 2: Prepare data in correct format\n# ####################################\ndef format_data(df):\n    df[\"text\"] = df.apply(\n        lambda x: f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n{x['reference']}<|im_end|>\\n\", \n        axis=1\n    )\n    return Dataset.from_pandas(df[[\"text\"]])\n\ntest_dataset = format_data(df_test)\n\ndef tokenize_function(examples):\n    tokenized = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\"\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# ####################################\n# STEP 3: Trainer for evaluate\n# ####################################\ntraining_args = TrainingArguments(\n    output_dir=\"./results_original_model\",\n    per_device_eval_batch_size=8, \n    fp16=True, \n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    eval_dataset=tokenized_test_dataset, \n)\n\n\n# ####################################\n# STEP 4: Evaluate and print the test loss\n# ####################################\nresults = trainer.evaluate()\nprint(f\"Mean Loss for the original model on the test set: {results['eval_loss']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T18:38:38.047153Z","iopub.execute_input":"2024-12-16T18:38:38.047892Z","iopub.status.idle":"2024-12-16T18:40:01.612057Z","shell.execute_reply.started":"2024-12-16T18:38:38.047856Z","shell.execute_reply":"2024-12-16T18:40:01.611183Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/518 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce0ea90cab542a49c33322ed5aa8ce5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [65/65 01:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Mean Loss for the original model on the test set: 6.530913352966309\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Our First fine-tuned model\n\nIn this section we do a finetuning using Transformer Trainer. This finetuning trains the entire model. It is a full finetuning approach\n\n### 1 The data is formatted for the chat task:\nFirst the data must be prepared in a precise format, at the moment we have decided, as for the other approaches to have a format delimited by <|im_start|> <|im_end|>.\n\n###  2 A pre-trained model is loaded \nWe then load the model with a reduced accuracy (FP16) as we have neither the resources nor the time to be able to use better accuracies. We tokenise train and test set.\n\n###  3 Transformer Trainer is applied for fine-tuning\nLoRa is configured for causal language modelling (CAUSAL_LM), with lora we add adapters to the model. In this way we can train few parameters compared to the total.\n\n###  4 We configure the trainer and start the training.\nWe then configure the trainer, using 1 batch_size, the accumulated gradient of 8. In this way we can simulate a batch_size of 8 (without making the training too heavy). This is because more and the programme crashes due to too much GPU usage. We also use the precision bfloat16 so as not to make the training too heavy\n\n### 5 We save the results and monitor the progress with W&B.","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport pandas as pd\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    Trainer, \n    TrainingArguments,\n    EarlyStoppingCallback\n)\nfrom datasets import Dataset\nimport wandb\n\nwandb.login()\nwandb.init(project=\"tiny-llama-finetuning\", name=\"experiment_1\")\n\nsplits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\ndf_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\ndf_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n\n\n# ####################################\n# STEP 1 we make data in correct format\n# STEP 2 We load the model\n# ####################################\n\ndef extract_prompt_and_reference(row):\n    parts = row.split(\"### Assistant:\")\n    prompt = parts[0].strip()\n    reference = parts[1].strip() if len(parts) > 1 else \"\"\n    return prompt, reference\n\ndf_train[[\"prompt\", \"reference\"]] = df_train[\"text\"].apply(lambda x: pd.Series(extract_prompt_and_reference(x)))\ndf_test[[\"prompt\", \"reference\"]] = df_test[\"text\"].apply(lambda x: pd.Series(extract_prompt_and_reference(x)))\n\ndef format_data(df):\n    df[\"text\"] = df.apply(\n        lambda x: f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n{x['reference']}<|im_end|>\", \n        axis=1\n    )\n    return Dataset.from_pandas(df[[\"text\"]])\n\ntrain_dataset = format_data(df_train)\ntest_dataset = format_data(df_test)\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"PY007/TinyLlama-1.1B-Chat-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"PY007/TinyLlama-1.1B-Chat-v0.1\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    config={\"dropout\": 0.1}\n)\n\ndef tokenize_function(examples):\n    tokenized = tokenizer(\n        examples[\"text\"], \n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntokenized_train_dataset = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ntokenized_test_dataset = test_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\n\n\n# ####################################\n# STEP 3 Train + Trainer\n# ####################################\n\ntraining_args = TrainingArguments(\n    output_dir=\"./finetuned_model\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    learning_rate=3e-5,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    fp16=False,\n    bf16=True,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"wandb\",\n    run_name=\"tiny-llama-run\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\ntrainer.train()\n\n\n\n# ####################################\n# STEP 4 Save\n# ####################################\n\nAttributeError.save_model(\"./finetuned_model\")\ntokenizer.save_pretrained(\"./finetuned_model\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:18:39.625668Z","iopub.execute_input":"2024-12-16T16:18:39.626373Z","iopub.status.idle":"2024-12-16T18:20:47.547966Z","shell.execute_reply.started":"2024-12-16T16:18:39.626325Z","shell.execute_reply":"2024-12-16T18:20:47.546944Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:ikgyhaxu) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">experiment_1</strong> at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/ikgyhaxu' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/ikgyhaxu</a><br/> View project at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241216_161813-ikgyhaxu/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:ikgyhaxu). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241216_161839-n0gvn5d8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/n0gvn5d8' target=\"_blank\">experiment_1</a></strong> to <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/n0gvn5d8' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/n0gvn5d8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9846 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f007a47af134d9f95c47c5c255882f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/518 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c7b4e4cb114d80a50b13b9167468a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1230/1230 2:01:37, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.450400</td>\n      <td>1.178330</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Fine-tuning completato e modello salvato!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# ####################################\n# STEP 1: Load model\n# ####################################\n\nmodel_path = \"./finetuned_model\"  \nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, \n    torch_dtype=torch.bfloat16,  \n    device_map=\"auto\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token \n\n# ####################################\n# STEP 2: Generate reosponse\n# ####################################\n\nprompt = \"What is the best programming language for Machine Learning?\"\nformatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100, \n        do_sample=True, \n        top_k=50, \n        top_p=0.7,\n        temperature=0.7,\n        repetition_penalty=1.1\n    )\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# ####################################\n# STEP 3: print result\n# ####################################\n\nprint(\"\\nPrompt:\")\nprint(prompt)\nprint(\"\\nRisposta Generata:\")\nprint(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T18:23:38.489701Z","iopub.execute_input":"2024-12-16T18:23:38.490076Z","iopub.status.idle":"2024-12-16T18:23:42.710963Z","shell.execute_reply.started":"2024-12-16T18:23:38.490042Z","shell.execute_reply":"2024-12-16T18:23:42.710110Z"}},"outputs":[{"name":"stdout","text":"\nPrompt:\nWhat is the best programming language for Machine Learning?\n\nRisposta Generata:\n<|im_start|>user\nWhat is the best programming language for Machine Learning?<|im_end|>\n<|im_start|>assistant\nThe best programming language for machine learning largely depends on your specific needs and goals. However, popular options include Python, R, Java, and Scala. Each has its own strengths and weaknesses in terms of ease of use, performance, and scalability. It's important to research and compare different programming languages to determine which one is best suited for your specific needs.### Human: What are some of the most popular Machine Learning frameworks available today?<|im_end|\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Second vertion of fine-tuning\n\nIn this section we do a finetuning using the Low-Rank Adaptation LoRA approach. <br>\n### 1 Data is formatted for the chat task:\nFirst the data must be prepared in a precise format, at the moment we have decided, as for the other approaches, to have a format delimited by <|im_start|> <|im_end|>.\n### 2 Load pre-trained\nThen the model is loaded with reduced precision (FP16), as we have neither the resources nor the time to be able to use better accuracies.\n\n### 3 LoRA\nLoRa is configured for causal language modelling (CAUSAL_LM), with which we add adapters to the model. In this way we can train few parameters compared to the total.\n\n### 4 Configuration for trainer + Train\nNext we configured the trainer, using 8 batch_sizes, the gradient accumulated by 4. In this way we can simulate a batch_size of 32 (without making the training too heavy). As another speedup we also have FP16 precision.\n\n### 5 Save model\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    TrainingArguments\n)\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import Dataset\nimport wandb\n\n\nwandb.init(project=\"tiny-llama-finetuning\", name=\"experiment_lora\")\nbase_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\noutput_dir = \"./finetuned_tinyllama_lora\"\n\n# ####################################\n# STEP 1 we make data in correct format\n# ####################################\n\ndef format_data(df):\n    df[\"text\"] = df.apply(\n        lambda x: f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n{x['reference']}<|im_end|>\\n\", \n        axis=1\n    )\n    return Dataset.from_pandas(df[[\"text\"]])\ntrain_dataset = format_data(df_train)\ntest_dataset = format_data(df_test)\n\n# ####################################\n# STEP 2 We load the model\n# ####################################\n    \ndef get_model_and_tokenizer(model_id):\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, \n        device_map=\"auto\", \n        torch_dtype=torch.float16 \n    )\n    \n    model.config.use_cache = False\n    return model, tokenizer\n\nmodel, tokenizer = get_model_and_tokenizer(base_model_id)\n\n\n# ####################################\n# STEP 3 LoRA\n# ####################################\npeft_config = LoraConfig(\n    r=8, \n    lora_alpha=16, \n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, peft_config)\n\n\n# ####################################\n# STEP 4 Training configuration + Trainer\n# ####################################\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    optim=\"adamw_torch\",\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    save_strategy=\"epoch\",\n    logging_steps=10,\n    fp16=True,\n    report_to=\"wandb\",\n    run_name=\"tiny-llama-run\",\n    max_steps=1500\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    max_seq_length=512, \n    dataset_text_field=\"text\",\n    args=training_args,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n\n\n# ####################################\n# STEP 5 Save the model\n# ####################################\n\ntrainer.model.save_pretrained(output_dir)\ntrainer.tokenizer.save_pretrained(output_dir)\n\nprint(f\"Modello fine-tunato salvato in {output_dir}\")\n\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T10:55:27.516270Z","iopub.execute_input":"2024-12-16T10:55:27.517130Z","iopub.status.idle":"2024-12-16T15:27:32.136635Z","shell.execute_reply.started":"2024-12-16T10:55:27.517093Z","shell.execute_reply":"2024-12-16T15:27:32.135736Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:6hojlicl) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">experiment_lora</strong> at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/6hojlicl' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/6hojlicl</a><br/> View project at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241216_105430-6hojlicl/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:6hojlicl). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241216_105527-lise0oh1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1' target=\"_blank\">experiment_lora</a></strong> to <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9846 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e4e982b6e4e44cab68e000464f446a2"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1500/1500 4:31:38, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.477000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.139600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.964000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.897400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.747100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.728700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.713800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.733500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.733200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.747800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.681600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.688000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.686100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.696700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.702500</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.665000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.727200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.657600</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.729200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.707700</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.666600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.702300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.703500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.684700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.690600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.692800</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.681800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.699600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.651200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.719900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.656200</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.718300</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.664300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.649400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.696600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.684300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.653000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.667300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.698900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.644700</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.659500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.664300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.709800</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.707900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.670100</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.681900</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.669700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.684200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.628800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.677400</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.671300</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.641100</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.690200</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.663000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.681000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.711000</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.685500</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.687300</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.678800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.668000</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.650900</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.659500</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.637100</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.689100</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.704500</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.694000</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.627400</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.643800</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.685600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.706900</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.674900</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.677300</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.678900</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.685100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.667800</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.642400</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.690100</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.637700</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.672800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.691700</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.652100</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.656900</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.631300</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.651900</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.681000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.661700</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.669200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.655000</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.633700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.670900</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.675800</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.699900</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.644800</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.654000</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.679400</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.681400</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.681500</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.673400</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.703000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.689900</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.619200</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.641100</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.659900</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.653200</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.656900</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.636100</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.655300</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.644600</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.645800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.671300</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.664200</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.620500</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.693500</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.650800</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.654200</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.662600</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.678000</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.665200</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.634700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.635900</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.663100</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.669100</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.681500</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.680000</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.653600</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.726500</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.654100</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.652900</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.663300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.682000</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.645500</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.674700</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.665600</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.649400</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.671400</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.645800</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.648400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.672400</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.662300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.678500</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.653400</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.647700</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.624600</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.669400</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.621500</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.650400</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.702200</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.625300</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.671700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.655100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"name":"stdout","text":"Modello fine-tunato salvato in ./finetuned_tinyllama_lora\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <style>\n        .wandb-row {\n            display: flex;\n            flex-direction: row;\n            flex-wrap: wrap;\n            justify-content: flex-start;\n            width: 100%;\n        }\n        .wandb-col {\n            display: flex;\n            flex-direction: column;\n            flex-basis: 100%;\n            flex: 1;\n            padding: 10px;\n        }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.4935145454855782e+17</td></tr><tr><td>train/epoch</td><td>4.87734</td></tr><tr><td>train/global_step</td><td>1500</td></tr><tr><td>train/grad_norm</td><td>0.86986</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6551</td></tr><tr><td>train_loss</td><td>0.68433</td></tr><tr><td>train_runtime</td><td>16308.9503</td></tr><tr><td>train_samples_per_second</td><td>2.943</td></tr><tr><td>train_steps_per_second</td><td>0.092</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">experiment_lora</strong> at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning/runs/lise0oh1</a><br/> View project at: <a href='https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning' target=\"_blank\">https://wandb.ai/corecs-universit-della-svizzera-italiana/tiny-llama-finetuning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241216_105527-lise0oh1/logs</code>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# Evaluation of our second model (fine-tuning with LoRA)\n\nIn this case, we do two things, the first is to assess whether the model works correctly, i.e. whether it correctly generates the response\n<br><br>\nNext, we test the loss on the test set to see how the model performs and validate whether it has actually learnt in the training.<br>\nFor this part we use the trainer, which is a convenient and similar implementation to the training.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n\n# ####################################\n# STEP 1 Load model + LoRa + tokenizer\n# ####################################\n\nbase_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\nadapter_path = \"./finetuned_tinyllama_lora\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id, \n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(model, adapter_path)\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token = tokenizer.eos_token  # Imposta il token di padding come EOS\n\n\n# ####################################\n# STEP 2 Generate a response in our format\n# ####################################\n\nprompt = \"What is the best programming language for Machine Learning?\"\nformatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100,\n        do_sample=True,\n        top_k=50,\n        top_p=0.7,\n        temperature=0.7,\n        repetition_penalty=1.1\n    )\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T15:48:37.913707Z","iopub.execute_input":"2024-12-16T15:48:37.914542Z","iopub.status.idle":"2024-12-16T15:48:43.875303Z","shell.execute_reply.started":"2024-12-16T15:48:37.914506Z","shell.execute_reply":"2024-12-16T15:48:43.874450Z"}},"outputs":[{"name":"stdout","text":"Caricamento del modello di base...\nCaricamento degli adattatori LoRA...\nCaricamento del tokenizer...\nEsempio di generazione del testo...\n\nRisposta Generata:\n<|im_start|>user\nWhat is the best programming language for Machine Learning?<|im_end|>\n<|im_start|>assistant\nThere are many programming languages used for machine learning, but the most common and popular choice is Python. Python is a high-level, versatile language that is known for its ease of use and widespread adoption in the AI industry. It has a large and active community of developers who develop and release new libraries and modules to further enhance the language's capabilities.\n\nOther popular choices for machine learning programming languages include R, Java, C++, C#, and Scala.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments\n)\nfrom peft import PeftModel\nfrom trl import SFTTrainer\nfrom datasets import Dataset\n\n\n\n# ####################################\n# STEP 1 Load model + LoRa + tokenizer\n# ####################################\n\nbase_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\nadapter_path = \"./finetuned_tinyllama_lora\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id, \n    torch_dtype=torch.float16, \n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(model, adapter_path)\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token = tokenizer.eos_token  \n\n\n\n# ####################################\n# STEP 2 make test in correct form\n# ####################################\n\ndef format_data(df):\n    df[\"text\"] = df.apply(\n        lambda x: f\"<|im_start|>user\\n{x['prompt']}<|im_end|>\\n<|im_start|>assistant\\n{x['reference']}<|im_end|>\\n\", \n        axis=1\n    )\n    return Dataset.from_pandas(df[[\"text\"]])\n\ntest_dataset = format_data(df_test)\n\n\n\n# ####################################\n# STEP 3 Trainer using for evaluation\n# ####################################\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_eval_batch_size=8,\n    fp16=True, \n    report_to=\"none\", \n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    dataset_text_field=\"text\",\n    max_seq_length=512,  \n)\n\n\n\n# ####################################\n# STEP 4 get the evaluation of our model\n# ####################################\n\nresults = trainer.evaluate()\nprint(f\"mean Loss on our test set: {results['eval_loss']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T15:54:25.665948Z","iopub.execute_input":"2024-12-16T15:54:25.666383Z","iopub.status.idle":"2024-12-16T15:55:48.344881Z","shell.execute_reply.started":"2024-12-16T15:54:25.666350Z","shell.execute_reply":"2024-12-16T15:55:48.344013Z"}},"outputs":[{"name":"stdout","text":"Caricamento del modello di base...\nCaricamento degli adattatori LoRA...\nCaricamento del tokenizer...\nConfigurazione del Trainer per la valutazione...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/518 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f52e6260c801446b9166e9cf39d3698b"}},"metadata":{}},{"name":"stdout","text":"Calcolo della loss sul test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [65/65 01:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Loss media sul test set: 1.8103272914886475\n","output_type":"stream"}],"execution_count":34}]}