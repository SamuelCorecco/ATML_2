{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:20:48.351090Z","iopub.execute_input":"2024-12-14T13:20:48.351752Z","iopub.status.idle":"2024-12-14T13:20:59.621413Z","shell.execute_reply.started":"2024-12-14T13:20:48.351717Z","shell.execute_reply":"2024-12-14T13:20:59.620325Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a4f40eecb0b74fc391378b60259ddf225dc0fdad668217c90dfd7433ab2fe58f\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import Dataset\nfrom tqdm import tqdm\n\nimport transformers\nimport torch\nimport pandas as pd\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:35:53.563617Z","iopub.execute_input":"2024-12-14T14:35:53.563967Z","iopub.status.idle":"2024-12-14T14:35:53.568825Z","shell.execute_reply.started":"2024-12-14T14:35:53.563939Z","shell.execute_reply":"2024-12-14T14:35:53.567989Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\nsplits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\ndf_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\ndf_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n\ndef extract_prompt_and_reference(row):\n    parts = row.split(\"### Assistant:\")\n    # Human prompt AND Assistant response is our output target\n    prompt = parts[0].strip()  \n    reference = parts[1].strip() if len(parts) > 1 else \"\" \n    return prompt, reference\n\ndf_train[[\"prompt\", \"reference\"]] = df_train[\"text\"].apply(lambda x: pd.Series(extract_prompt_and_reference(x)))\ndf_test[[\"prompt\", \"reference\"]] = df_test[\"text\"].apply(lambda x: pd.Series(extract_prompt_and_reference(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:19:15.704655Z","iopub.execute_input":"2024-12-14T13:19:15.704983Z","iopub.status.idle":"2024-12-14T13:19:17.752829Z","shell.execute_reply.started":"2024-12-14T13:19:15.704955Z","shell.execute_reply":"2024-12-14T13:19:17.752091Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Checkpoint Evaluation\nthe first thing to do is to understand how our practice model is set up, following what they did on huggingface the model uses https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.1 as a base, so before finetuning we want to understand how the model performs","metadata":{}},{"cell_type":"code","source":"model = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device=0 if device == \"cuda\" else -1,  # GPU: device=0, CPU: device=-1\n)\n\nprompt = \"which anime is the most important one\"\nformatted_prompt = f\"### Human: {prompt} ### Assistant:\"\nsequences = pipeline(\n    formatted_prompt,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    repetition_penalty=1.5,\n    eos_token_id=tokenizer.eos_token_id,\n    truncation=True,\n    max_length=500,\n)\nfor seq in sequences:\n    print(seq[\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:27:12.490647Z","iopub.execute_input":"2024-12-14T13:27:12.491447Z","iopub.status.idle":"2024-12-14T13:27:15.954914Z","shell.execute_reply.started":"2024-12-14T13:27:12.491412Z","shell.execute_reply":"2024-12-14T13:27:15.953906Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"Both `max_new_tokens` (=32) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"### Human: which anime is the most important one ### Assistant: The Dragon Ball Z series.### Speaker 2: That's a tough question, there are so many great ones out there!\nIn\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"dataset = Dataset.from_pandas(df_test)\n\n\ndef generate_response(example):\n    # Formatta il prompt\n    formatted_prompt = f\"### Human: {example['prompt']} ### Assistant:\"\n    \n    # Calcola la lunghezza del prompt in token\n    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\")[\"input_ids\"]\n    prompt_length = input_ids.shape[1]  # Numero di token nel prompt\n    \n    # Imposta max_length come la lunghezza del prompt + margine per la generazione\n    max_length = prompt_length + 100  # Aggiungi un margine di 100 token per la generazione\n    \n    # Genera la risposta\n    sequences = pipeline(\n        formatted_prompt,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        repetition_penalty=1.5,\n        eos_token_id=tokenizer.eos_token_id,\n        truncation=True,\n        max_length=max_length,  # Lunghezza dinamica\n        max_new_tokens=None  # Imposta esplicitamente a None per evitare conflitti\n    )\n    \n    # Pulisci il testo generato\n    generated_text = sequences[0][\"generated_text\"] if sequences else \"\"\n    parts = generated_text.split(\"### Assistant:\")\n    cleaned_response = parts[1].strip() if len(parts) > 1 else \"\"\n    return {\"generated\": cleaned_response}\n\ndataset = dataset.map(generate_response, batched=False)\n\n\n# Funzione per calcolare BLEU\ndef calculate_bleu(reference, candidate):\n    reference_tokens = reference.split()\n    candidate_tokens = candidate.split()\n    # Usa SmoothingFunction per evitare warning\n    smoothing_function = SmoothingFunction().method1\n    return sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothing_function)\n\n\n# Funzione per calcolare ROUGE\nscorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n\ndef calculate_rouge_score(example):\n    scores = scorer.score(example['reference'], example['generated'])\n    return {\n        \"rouge1\": scores[\"rouge1\"].fmeasure,\n        \"rouge2\": scores[\"rouge2\"].fmeasure,\n        \"rougeL\": scores[\"rougeL\"].fmeasure,\n    }\n# Calcola BLEU\ndataset = dataset.map(calculate_bleu_score)\n\n# Calcola ROUGE\ndataset = dataset.map(calculate_rouge_score)\n\naverage_bleu = sum(dataset[\"bleu\"]) / len(dataset)\naverage_rouge1 = sum(dataset[\"rouge1\"]) / len(dataset)\naverage_rouge2 = sum(dataset[\"rouge2\"]) / len(dataset)\naverage_rougeL = sum(dataset[\"rougeL\"]) / len(dataset)\n\nprint(\"Average BLEU:\", average_bleu)\nprint(\"Average ROUGE-1:\", average_rouge1)\nprint(\"Average ROUGE-2:\", average_rouge2)\nprint(\"Average ROUGE-L:\", average_rougeL)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:36:09.536214Z","iopub.execute_input":"2024-12-14T14:36:09.536559Z","iopub.status.idle":"2024-12-14T14:36:13.372729Z","shell.execute_reply.started":"2024-12-14T14:36:09.536530Z","shell.execute_reply":"2024-12-14T14:36:13.371755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/518 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73890cd2e69b4f7bb7a4b5882c8f921e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/518 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6642ef4bb64c4a2e9e89b77edd1185cd"}},"metadata":{}},{"name":"stdout","text":"Average BLEU: 0.1623378038711583\nAverage ROUGE-1: 0.15047413918509633\nAverage ROUGE-2: 0.016966875004574115\nAverage ROUGE-L: 0.09186045851931779\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Baseline ","metadata":{}},{"cell_type":"code","source":"model = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device=0 if device == \"cuda\" else -1,  # GPU: device=0, CPU: device=-1\n)\n\n\nprompt = \"What do you think of Pokemon?\"\nformatted_prompt = (\n    f\"### Human: {prompt}### Assistant:\"\n)\n\n\nsequences = pipeline(\n    formatted_prompt,\n    do_sample=True,\n    top_k=50,\n    top_p = 0.7,\n    num_return_sequences=1,\n    repetition_penalty=1.1,\n    max_new_tokens=500,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}