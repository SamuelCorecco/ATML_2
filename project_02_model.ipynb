{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samu/Desktop/Usi/Advanced Topics in Machine Learning/ATML_2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge_score'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n",
    "def extract_prompt_and_reference(row):\n",
    "    parts = row.split(\"### Assistant:\")\n",
    "    # Human prompt AND Assistant response is our output target\n",
    "    prompt = parts[0].strip()  \n",
    "    reference = parts[1].strip() if len(parts) > 1 else \"\" \n",
    "    return prompt, reference\n",
    "\n",
    "df_train[[\"prompt\", \"reference\"]] = df_train[\"text\"].apply(lambda x: pd.Series(extract_prompt_and_reference(x)))\n",
    "df_test[[\"prompt\", \"reference\"]] = df_test[\"text\"].apply(lambda x: pd.Series(extract_prompt_and_reference(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Evaluation\n",
    "the first thing to do is to understand how our practice model is set up, following what they did on huggingface the model uses https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.1 as a base, so before finetuning we want to understand how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0 if device == \"cuda\" else -1,  # GPU: device=0, CPU: device=-1\n",
    ")\n",
    "\n",
    "prompt = \"which anime is the most important one\"\n",
    "formatted_prompt = f\"### Human: {prompt} ### Assistant:\"\n",
    "sequences = pipeline(\n",
    "    formatted_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.5,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    truncation=True,\n",
    "    max_length=500,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(seq[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "generated_responses = []\n",
    "for prompt in tqdm(df_test[\"prompt\"], desc=\"Generating responses\"):\n",
    "    formatted_prompt = f\"### Human: {prompt} ### Assistant:\"\n",
    "    sequences = pipeline(\n",
    "        formatted_prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        truncation=True,\n",
    "        max_length=500,\n",
    "    )\n",
    "    generated_responses.append(sequences[0][\"generated_text\"] if sequences else \"\")\n",
    "    \n",
    "def clean_generated_response(response):\n",
    "    parts = response.split(\"### Assistant:\")\n",
    "    return parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "df_test[\"generated\"] = [clean_generated_response(resp) for resp in generated_responses]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference_tokens = reference.split()\n",
    "    candidate_tokens = candidate.split()\n",
    "    return sentence_bleu([reference_tokens], candidate_tokens)\n",
    "\n",
    "df_test[\"bleu\"] = df_test.apply(lambda row: calculate_bleu(row[\"reference\"], row[\"generated\"]), axis=1)\n",
    "print(\"Average BLEU score:\", df_test[\"bleu\"].mean())\n",
    "\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure\n",
    "\n",
    "df_test[[\"rouge1\", \"rouge2\", \"rougeL\"]] = df_test.apply(\n",
    "    lambda row: pd.Series(calculate_rouge(row[\"reference\"], row[\"generated\"])), axis=1\n",
    ")\n",
    "print(\"Average ROUGE-1:\", df_test[\"rouge1\"].mean())\n",
    "print(\"Average ROUGE-2:\", df_test[\"rouge2\"].mean())\n",
    "print(\"Average ROUGE-L:\", df_test[\"rougeL\"].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device=0 if device == \"cuda\" else -1,  # GPU: device=0, CPU: device=-1\n",
    ")\n",
    "\n",
    "\n",
    "prompt = \"What do you think of Pokemon?\"\n",
    "formatted_prompt = (\n",
    "    f\"### Human: {prompt}### Assistant:\"\n",
    ")\n",
    "\n",
    "\n",
    "sequences = pipeline(\n",
    "    formatted_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p = 0.7,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=500,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
