{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T21:31:54.837225Z",
     "iopub.status.busy": "2025-01-11T21:31:54.836887Z",
     "iopub.status.idle": "2025-01-11T21:32:34.415538Z",
     "shell.execute_reply": "2025-01-11T21:32:34.414470Z",
     "shell.execute_reply.started": "2025-01-11T21:31:54.837197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install rouge-score\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install bitsandbytes\n",
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T21:33:16.170100Z",
     "iopub.status.busy": "2025-01-11T21:33:16.169306Z",
     "iopub.status.idle": "2025-01-11T21:33:16.176727Z",
     "shell.execute_reply": "2025-01-11T21:33:16.175410Z",
     "shell.execute_reply.started": "2025-01-11T21:33:16.170065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import lightning as L\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import TrainerCallback\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T16:42:56.150469Z",
     "iopub.status.busy": "2025-01-11T16:42:56.150135Z",
     "iopub.status.idle": "2025-01-11T16:42:56.646717Z",
     "shell.execute_reply": "2025-01-11T16:42:56.646070Z",
     "shell.execute_reply.started": "2025-01-11T16:42:56.150441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n",
    "first_row = df_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Evaluation\n",
    "the first thing to do is to understand how our practice model is set up, following what they did on huggingface the model uses https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.1 as a base, so before finetuning we want to understand how the model performs.<br><br>\n",
    "The idea is to see how it handles the questions in the dataset format and also we want to see how it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T16:39:10.197619Z",
     "iopub.status.busy": "2025-01-11T16:39:10.197296Z",
     "iopub.status.idle": "2025-01-11T16:39:37.165932Z",
     "shell.execute_reply": "2025-01-11T16:39:37.164976Z",
     "shell.execute_reply.started": "2025-01-11T16:39:10.197592Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b9c79ea6e443b5aef931ce58d410bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325263450f544d8b9bce8eb620b27c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59733e71dd7447bf80b0168a32e873ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: What is the best programming language for Machine Learning? ### Assistant: Can you tell us more about your background and how does it help in understanding machine learning problem.  #### Q1A:- How would someone be able to answer\n"
     ]
    }
   ],
   "source": [
    "model = \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0 if device == \"cuda\" else -1,  # GPU: device=0, CPU: device=-1\n",
    ")\n",
    "\n",
    "prompt = \"What is the best programming language for Machine Learning?\"\n",
    "formatted_prompt = f\"### Human: {prompt} ### Assistant:\"\n",
    "sequences = pipeline(\n",
    "    formatted_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.5,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=32,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(seq[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T16:44:07.229278Z",
     "iopub.status.busy": "2025-01-11T16:44:07.228939Z",
     "iopub.status.idle": "2025-01-11T16:45:29.651489Z",
     "shell.execute_reply": "2025-01-11T16:45:29.650705Z",
     "shell.execute_reply.started": "2025-01-11T16:44:07.229257Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bbd0d913904d4f915c5f3e36ba3b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss for the original model on the test set: 7.8757123947143555\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# ####################################\n",
    "# STEP 1: Load model and tokenizer\n",
    "# ####################################\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# ####################################\n",
    "# STEP 2: Prepare data in correct format\n",
    "# ####################################\n",
    "\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# ####################################\n",
    "# STEP 3: Trainer for evaluate\n",
    "# ####################################\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_original_model\",\n",
    "    per_device_eval_batch_size=8, \n",
    "    fp16=True, \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_test_dataset, \n",
    ")\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 4: Evaluate and print the test loss\n",
    "# ####################################\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"Mean Loss for the original model on the test set: {results['eval_loss']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌟 Our complete Fine-Tuned Model\n",
    "\n",
    "In this section, we demonstrate the process of fine-tuning a Transformer model (`TinyLlama-1.1B-step-50K-105b`) using the `Trainer` from Hugging Face. This is a **full fine-tuning approach**, where the entire model is trained.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Steps to Fine-Tune the Model\n",
    "\n",
    "### 1️⃣ **Data Preparation for the Chat Task**\n",
    "To begin, the data already in the corrected  formatted suitable for chat-based tasks. We use a delimiter format that incorporates:\n",
    "- `### Human`: prompt\n",
    "- `### Assistant`: the response\n",
    "\n",
    "This structured format is both the training and evaluation datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **Loading a Pre-Trained Model**\n",
    "We load a pre-trained model configured with reduced precision (**FP16**) to manage resource constraints (we have max 16 GB or GPU). This allows us to perform efficient fine-tuning while balancing accuracy and performance.\n",
    "\n",
    "The training and test datasets are tokenized with a maximum sequence length of 512 to ensure compatibility with the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ **Applying Transformer Trainer for Fine-Tuning**\n",
    "We perform **full fine-tuning** of the model, meaning all the model's parameters are updated during training. This approach ensures the model is fully adapted to the task at hand.\n",
    "\n",
    "The training task is configured for **causal language modeling (CAUSAL_LM)**, which is suitable for autoregressive tasks like chat-based interactions. Tokenized datasets for training and testing are fed directly into the `Trainer`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ **Trainer Configuration and Training**\n",
    "The `Trainer` is configured with the following settings:\n",
    "- **Batch Size**: We use a `per_device_train_batch_size` of 1 to minimize memory usage.\n",
    "- **Gradient Accumulation**: By accumulating gradients over **8** steps, we simulate a batch size of 8.\n",
    "- **Precision**: We adopt `bfloat16` precision to further reduce the GPU load without sacrificing too much accuracy.\n",
    "- **Learning Rate**: A low learning rate (`3e-5`) is used to ensure stable convergence.\n",
    "\n",
    "This configuration balances efficiency and accuracy, allowing us to train the model effectively within hardware constraints.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎉 **Results**\n",
    "This setup, we've successfully fine-tuned our model and our loss on test test is **1.25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:00:26.030854Z",
     "iopub.status.busy": "2025-01-11T17:00:26.030574Z",
     "iopub.status.idle": "2025-01-11T17:00:26.034641Z",
     "shell.execute_reply": "2025-01-11T17:00:26.033836Z",
     "shell.execute_reply.started": "2025-01-11T17:00:26.030833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T19:21:56.122517Z",
     "iopub.status.busy": "2025-01-11T19:21:56.121867Z",
     "iopub.status.idle": "2025-01-11T21:25:04.187508Z",
     "shell.execute_reply": "2025-01-11T21:25:04.186387Z",
     "shell.execute_reply.started": "2025-01-11T19:21:56.122487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7425151dce3d466cabbb28642584d36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a97ea3fc9d4c04bf5ea31aaa6c4bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf89a155d2e74fa98ab9d33a97fa9945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5481fe77ce4485ab882b1c4716073e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ae6820983e4da38493004eda8b2df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421a73345a6d4726abba7a34460137ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440fc9f2b5cf4409aff4288fb95ab9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a319cf0281f4770b403cf886da70836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89222f2ce511403ea93c08ff3b3e0aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1230/1230 2:02:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>10.133200</td>\n",
       "      <td>1.265551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>10.057000</td>\n",
       "      <td>1.259194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_model/tokenizer_config.json',\n",
       " './finetuned_model/special_tokens_map.json',\n",
       " './finetuned_model/tokenizer.model',\n",
       " './finetuned_model/added_tokens.json',\n",
       " './finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 1 we make data in correct format\n",
    "# STEP 2 We load the model\n",
    "# ####################################\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-step-50K-105b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    config={\"dropout\": 0.1}\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 Train + Trainer\n",
    "# ####################################\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=3000,\n",
    "    eval_steps=500, \n",
    "    evaluation_strategy=\"steps\", \n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ####################################\n",
    "# STEP 4 Save\n",
    "# ####################################\n",
    "\n",
    "trainer.save_model(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌟 QLoRA Fine-Tuning Approach\n",
    "\n",
    "In this project, we fine-tune a Transformer model (`TinyLlama-1.1B-step-50K-105b`) using **Quantized Low-Rank Adaptation (QLoRA)**. This approach is  efficient, with a **4-bit quantization** and **LoRA adapters** we can reduce memory usage and computational cost. \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Steps to Fine-Tune the Model with QLoRA\n",
    "\n",
    "### 1️⃣ **Data Formatting**\n",
    "As in the previous approach, the dataset is pre-formatted for chat-based tasks. The structured format includes:\n",
    "- **`### Human`**: Marks the user's prompt.\n",
    "- **`### Assistant`**: Marks the assistant's response.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **Model Loading with 4-Bit Quantization**\n",
    "To optimize memory usage and computational efficiency, the base model is loaded with **4-bit quantization** using the following settings:\n",
    "- **Quantization Type**: `NF4` (Normalized Float 4) for improved precision.\n",
    "- **Compute Precision**: `bfloat16` for efficient computations on modern GPUs.\n",
    "- **Double Quantization**: Enabled to further reduce memory requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ **LoRA Configuration**\n",
    "We apply **Low-Rank Adaptation (LoRA)** to add trainable adapters to the model. The configuration includes:\n",
    "- **Rank (r)**: 8\n",
    "- **Alpha**: 16\n",
    "- **Dropout**: 0.05\n",
    "- **Bias**: None \n",
    "\n",
    "The model is prepared for k-bit training using `prepare_model_for_kbit_training` before applying LoRA with `get_peft_model`. This is used to configure the model when using quantisation (4-bit or 8-bit). Sets the model to have quantised weights and prevents certain parts from being quantised instead (e.g. embedding)\n",
    "Also disables traiing in some parts of the model\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ **Trainer Configuration and Training**\n",
    "The fine-tuning process is acived  using `PyTorch Lightning` . The configuration includes:\n",
    "- **Batch Size**: 4 with gradient accumulation over 64 steps\n",
    "- **Precision**: Mixed `bfloat16` (optimize GPU memory usage)\n",
    "- **Gradient Clipping**: 1.0 (prevent exploding gradients)\n",
    "- **Epochs**: 1\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ **Saving the Fine-Tuned Model**\n",
    "The fine-tuned model and tokenizer are saved locally in the directory `./finetuned_qlora_model`. This enables easy reuse and evaluation in future experiments.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎉 **Results**\n",
    "The fine-tuning process achieved a **test loss of 2.18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T21:33:46.513069Z",
     "iopub.status.busy": "2025-01-11T21:33:46.512716Z",
     "iopub.status.idle": "2025-01-11T22:51:42.353083Z",
     "shell.execute_reply": "2025-01-11T22:51:42.351977Z",
     "shell.execute_reply.started": "2025-01-11T21:33:46.513041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e58cc2755f494f982623da35cdabca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de43ac8a17e4a3e9629b3b446ddb69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a1f975e20143998ed823d2a02df186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c1f07aef91470a92bb84e6f8ac871b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f3f9f987bf49d5b731f2a30bd1905f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fbd4bebb9f4d4eb5abc1b49dc415f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca57a92044041199c0a5b36db726b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 616 M  | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "615 M     Non-trainable params\n",
      "616 M     Total params\n",
      "2,466.931 Total estimated model params size (MB)\n",
      "442       Modules in train mode\n",
      "293       Modules in eval mode\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f65caa369243d592b2ccfad6af18d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_qlora_model/tokenizer_config.json',\n",
       " './finetuned_qlora_model/special_tokens_map.json',\n",
       " './finetuned_qlora_model/tokenizer.model',\n",
       " './finetuned_qlora_model/added_tokens.json',\n",
       " './finetuned_qlora_model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 2 Load quantizate model\n",
    "# ####################################\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "\n",
    "train_formatted = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "test_formatted = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "train_dataset = Dataset.from_dict({\"text\": train_formatted[\"text\"].tolist()})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_formatted[\"text\"].tolist()})\n",
    "\n",
    "# train_dataset = Dataset.from_dict({\"text\": train_formatted})\n",
    "# test_dataset = Dataset.from_dict({\"text\": test_formatted})\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  \n",
    "    bnb_4bit_quant_type=\"nf4\",              \n",
    "    bnb_4bit_use_double_quant=True          \n",
    ")\n",
    "\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",          \n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 LoRa\n",
    "# ####################################\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model) \n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        [sample['text'] for sample in batch], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    labels = inputs.input_ids.clone()\n",
    "    labels[~inputs.attention_mask.bool()] = -100\n",
    "    return inputs, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    collate_fn=collate_fn, \n",
    "    shuffle=True, \n",
    "    batch_size=4, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# just as we see in class\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "        # Shift logits to exclude the last element\n",
    "        # shift labels to exclude the first element\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        # Compute LM loss token-wise\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 4 Trainer + Train\n",
    "# ####################################\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=64,\n",
    "    precision=\"bf16-mixed\", \n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=1\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 5 Save\n",
    "# ####################################\n",
    "\n",
    "model.save_pretrained(\"./finetuned_qlora_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_qlora_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T22:52:01.966124Z",
     "iopub.status.busy": "2025-01-11T22:52:01.965736Z",
     "iopub.status.idle": "2025-01-11T22:54:04.287952Z",
     "shell.execute_reply": "2025-01-11T22:54:04.287044Z",
     "shell.execute_reply.started": "2025-01-11T22:52:01.966086Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8c2b9da90c4fc4aa9cfe8578bc9de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.185150384902954     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.185150384902954    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 2.185150384902954}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LightningWrapper2(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "        \n",
    "lightning_model_test = LightningWrapper2(model)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    collate_fn=collate_fn, \n",
    "    shuffle=False, \n",
    "    batch_size=2, \n",
    "    num_workers=2\n",
    ")\n",
    "trainer.test(lightning_model_test, dataloaders=test_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR MAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First fine tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "The best programming language for machine learning is Python. Python has a wide range of features and capabilities that make it a popular choice for machine learning developers. Here are some of the most popular features of Python:\n",
      "\n",
      "- Fast development: Python is a general purpose language that can be used to develop applications for various platforms, including web servers, mobile devices, and embedded systems. It's also a good choice for developing machine learning algorithms that require high performance and efficiency.\n",
      "\n",
      "- E\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# ####################################\n",
    "# STEP 1: load model + tokenizer\n",
    "# ####################################\n",
    "\n",
    "model_path = \"Models/finetuned_model_2_1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": device} \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ####################################\n",
    "# STEP 2: Genera la risposta\n",
    "# ####################################\n",
    "\n",
    "prompt = \"What is the best programming language for Machine Learning?\"\n",
    "formatted_prompt = f\"### Human: {prompt} ### Assistant: \"\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100, \n",
    "        do_sample=True, \n",
    "        top_k=50, \n",
    "        top_p=0.7,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_response = response.split(\"### Assistant:\")[-1].strip()\n",
    "generated_response = generated_response.split(\"###\")[0].strip()\n",
    "\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(generated_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: 🤖 It's a difficult question to answer, but we are currently in the process of creating a list of languages that we feel have the best chance of being used by machine learning professionals. If you have any suggestions, please leave them below!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_path = \"Models/finetuned_qlora_model_2\"\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float32, \n",
    "    device_map=None\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"What is the best programming language for Machine Learning?\"\n",
    "prompt_2 = f\"### Human: {prompt} ### Assistant: \"\n",
    "\n",
    "inputs = tokenizer(prompt_2, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=30,\n",
    "        top_p=0.7,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_response = response.split(\"### Assistant:\")[-1].strip()\n",
    "generated_response = generated_response.split(\"###\")[0].strip()\n",
    "print(\"Generated response:\", generated_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌟 Interactive Chatbot with Fine-Tuned Model\n",
    "\n",
    "This script demonstrates how to load a fine-tuned model for interactive chatbot functionality. It leverages a **Transformer-based model** trained on a chat-style dataset and allows for real-time user interaction.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Key Steps in the Script\n",
    "\n",
    "### 1️⃣ **Environment and Device Setup**\n",
    "The script dynamically selects the device for computation:\n",
    "- **MPS** (Metal Performance Shaders) if available (for macOS).\n",
    "- **CPU** as a fallback if MPS is not supported.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **How to Interact with the Model**\n",
    "\n",
    "The script runs an interactive loop where the user can type questions or prompts, and the fine-tuned model generates a response. Here's how it works:\n",
    "\n",
    "1. **User Input**:\n",
    "   - The user enters their message into the console. \n",
    "   - To end the interaction, the user can type `esc` or `end`.\n",
    "\n",
    "2. **Prompt Construction**:\n",
    "   - you can message as you wish, the code handles the format\n",
    "   - If desired, `MEMORY_SAVING` can be enabled to allow the model to see the previous chat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samu/Desktop/Usi/Advanced Topics in Machine Learning/ATML_2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/samu/Desktop/Usi/Advanced Topics in Machine Learning/ATML_2/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you: hello\n",
      "Bot: Hello! How are you?\n",
      "you: how do I import TensorFlow into Python?\n",
      "Bot: To import TensorFlow into Python, you can follow these steps:\n",
      "\n",
      "1. Install TensorFlow by running the following command:\n",
      "\n",
      "```bash\n",
      "pip install tensorflow\n",
      "```\n",
      "\n",
      "2. Import TensorFlow by running the following\n",
      "you: I get the error: `ModuleNotFoundError: No module named 'tensorflow'`. What should I do?\n",
      "Bot: The error you're seeing is because TensorFlow is not installed on your system. You can install TensorFlow by running the following command:\n",
      "\n",
      "```\n",
      "pip install tensorflow\n",
      "```\n",
      "\n",
      "This will install TensorFlow on your system\n",
      "you: perfect, everything worked thank you very much. Bye\n",
      "Bot: Thank you very much for your kind words. I'm glad you enjoyed it.\n",
      "you: esc\n",
      "\n",
      "\n",
      "END :)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "MEMORY_SAVING = False\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_path = \"Models/finetuned_model_2_1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": device}\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "prev_prompt = \"\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    print(f\"you: {user_input}\")\n",
    "    if user_input.lower() == \"esc\" or user_input.lower() == \"end\" :\n",
    "        print(\"\\n\\nEND :)\")\n",
    "        break\n",
    "\n",
    "    prompt = f\"### Human: {user_input} ### Assistant:\"\n",
    "    prompt = prev_prompt + prompt\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if MEMORY_SAVING:\n",
    "        response = response.split(prev_prompt)[1].strip()\n",
    "\n",
    "    response = response.split(\"### Assistant:\")[1].split(\"###\")[0].strip()\n",
    "\n",
    "    if MEMORY_SAVING:\n",
    "        prev_prompt += f\"### Human: {user_input} ### Assistant: {response} ###\"\n",
    "    \n",
    "    print(f\"Bot: {response}\")\n",
    "\n",
    "#hello\n",
    "#How do I import TensorFlow into Python?\n",
    "#I get the error: `ModuleNotFoundError: No module named 'tensorflow'`. What should I do?\n",
    "#perfect, everything worked thank you very much. Bye\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
