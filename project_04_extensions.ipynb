{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T18:38:42.608146Z",
     "iopub.status.busy": "2024-12-17T18:38:42.607680Z",
     "iopub.status.idle": "2024-12-17T18:39:11.943830Z",
     "shell.execute_reply": "2024-12-17T18:39:11.942818Z",
     "shell.execute_reply.started": "2024-12-17T18:38:42.608116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-17T18:41:55.405427Z",
     "iopub.status.busy": "2024-12-17T18:41:55.404761Z",
     "iopub.status.idle": "2024-12-17T18:41:55.410393Z",
     "shell.execute_reply": "2024-12-17T18:41:55.409489Z",
     "shell.execute_reply.started": "2024-12-17T18:41:55.405389Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samu/Desktop/Usi/Advanced Topics in Machine Learning/ATML_2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import lightning as L\n",
    "import gc\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T18:40:02.917744Z",
     "iopub.status.busy": "2024-12-17T18:40:02.917094Z",
     "iopub.status.idle": "2024-12-17T18:40:04.434936Z",
     "shell.execute_reply": "2024-12-17T18:40:04.434018Z",
     "shell.execute_reply.started": "2024-12-17T18:40:02.917710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Describe this anime', 'input': 'Cowboy Bebop', 'output': \"Crime is timeless. By the year 2071, humanity has expanded across the galaxy, filling the surface of other planets with settlements like those on Earth. These new societies are plagued by murder, drug use, and theft, and intergalactic outlaws are hunted by a growing number of tough bounty hunters.\\n\\nSpike Spiegel and Jet Black pursue criminals throughout space to make a humble living. Beneath his goofy and aloof demeanor, Spike is haunted by the weight of his violent past. Meanwhile, Jet manages his own troubled memories while taking care of Spike and the Bebop, their ship. The duo is joined by the beautiful con artist Faye Valentine, odd child Edward Wong Hau Pepelu Tivrusky IV, and Ein, a bioengineered Welsh Corgi.\\n\\nWhile developing bonds and working to catch a colorful cast of criminals, the Bebop crew's lives are disrupted by a menace from Spike's past. As a rival's maniacal plot continues to unravel, Spike must choose between life with his newfound family or revenge for his old wounds.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET_PATH = \"/kaggle/input/animedataset/anime-dataset-2023.csv\"\n",
    "\n",
    "anime_df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "def prepare_alpaca_format_name_only(df):\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        instruction = \"Describe this anime\"\n",
    "        input_text = row['Name']\n",
    "        output_text = row['Synopsis']\n",
    "        \n",
    "        example = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        }\n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "alpaca_dataset_name_only = prepare_alpaca_format_name_only(anime_df)\n",
    "\n",
    "print(alpaca_dataset_name_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T20:03:08.089574Z",
     "iopub.status.busy": "2024-12-17T20:03:08.089240Z",
     "iopub.status.idle": "2024-12-17T21:21:19.224075Z",
     "shell.execute_reply": "2024-12-17T21:21:19.223244Z",
     "shell.execute_reply.started": "2024-12-17T20:03:08.089548Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints exists and is not empty.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 616 M  | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "615 M     Non-trainable params\n",
      "616 M     Total params\n",
      "2,466.947 Total estimated model params size (MB)\n",
      "442       Modules in train mode\n",
      "315       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558508af6d5645e3a05507b6768701b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_qlora_model/tokenizer_config.json',\n",
       " './finetuned_qlora_model/special_tokens_map.json',\n",
       " './finetuned_qlora_model/tokenizer.model',\n",
       " './finetuned_qlora_model/added_tokens.json',\n",
       " './finetuned_qlora_model/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_data = alpaca_dataset_name_only\n",
    "\n",
    "# Trasformiamo il dataset in un formato compatibile\n",
    "train_formatted = [f\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\" for sample in alpaca_data]\n",
    "\n",
    "# Creiamo il dataset HuggingFace\n",
    "train_dataset = Dataset.from_dict({\"text\": train_formatted})\n",
    "\n",
    "# ####################################\n",
    "# STEP 2.1: Configurazione della quantizzazione\n",
    "# ####################################\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Carichiamo il modello in 4-bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Utilizziamo bfloat16 per i calcoli\n",
    "    bnb_4bit_quant_type=\"nf4\",              # Tipo di quantizzazione\n",
    "    bnb_4bit_use_double_quant=True          # Abilitiamo la doppia quantizzazione\n",
    ")\n",
    "\n",
    "# ####################################\n",
    "# STEP 2.2: Caricamento del Modello e Tokenizer\n",
    "# ####################################\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "\n",
    "# Carichiamo il tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Carichiamo il modello con la configurazione quantizzata\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",          # Mappatura automatica sui dispositivi\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ####################################\n",
    "# STEP 3: Configurazione LoRa\n",
    "# ####################################\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                       # Grado di fattorizzazione per LoRa\n",
    "    lora_alpha=16,             # Valore di alpha\n",
    "    lora_dropout=0.05,         # Dropout per regolarizzare\n",
    "    bias=\"none\",              # Nessuna modifica ai bias\n",
    "    task_type=\"CAUSAL_LM\"      # Modello di linguaggio causale\n",
    ")\n",
    "\n",
    "# Prepariamo il modello per l'addestramento k-bit\n",
    "model = prepare_model_for_kbit_training(model) \n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ####################################\n",
    "# STEP 4: DataLoader\n",
    "# ####################################\n",
    "def collate_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        [sample['text'] for sample in batch], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=206\n",
    "    )\n",
    "    labels = inputs.input_ids.clone()\n",
    "    labels[~inputs.attention_mask.bool()] = -100\n",
    "    return inputs, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    collate_fn=collate_fn, \n",
    "    shuffle=True, \n",
    "    batch_size=8, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# ####################################\n",
    "# STEP 4.1: Lightning Wrapper\n",
    "# ####################################\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "        # Shift logits to exclude the last element\n",
    "        # shift labels to exclude the first element\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        # Compute LM loss token-wise\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model)\n",
    "\n",
    "# ####################################\n",
    "# STEP 4.2: Trainer + Train\n",
    "# ####################################\n",
    "\n",
    "# Callback per il salvataggio del checkpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints\",  \n",
    "    filename=\"finetuned_model-{epoch:02d}-{train_loss:.2f}\", \n",
    "    save_top_k=-1,            \n",
    "    save_last=True,          \n",
    "    monitor=\"train_loss\",    \n",
    "    mode=\"min\"              \n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=32,\n",
    "    precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=1,\n",
    "    callbacks=[checkpoint_callback],  # Aggiungi il callback per i checkpoint\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Save the Fine-tuned Model\n",
    "# ####################################\n",
    "model.save_pretrained(\"./finetuned_qlora_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_qlora_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Caricamento del checkpoint\n",
    "checkpoint_path = \"./checkpoints/last.ckpt\"  # Percorso del checkpoint più recente\n",
    "\n",
    "# Carica il modello Lightning dal checkpoint\n",
    "lightning_model = LightningWrapper.load_from_checkpoint(checkpoint_path, model=model)\n",
    "\n",
    "# Crea un nuovo Trainer\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=32,\n",
    "    precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=5  # Puoi aumentare il numero di epoch per continuare\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:48:56.965723Z",
     "iopub.status.busy": "2024-12-17T21:48:56.965336Z",
     "iopub.status.idle": "2024-12-17T21:58:33.713826Z",
     "shell.execute_reply": "2024-12-17T21:58:33.712915Z",
     "shell.execute_reply.started": "2024-12-17T21:48:56.965689Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/checkpoints/ (stored 0%)\n",
      "  adding: kaggle/working/checkpoints/last.ckpt (deflated 22%)\n",
      "  adding: kaggle/working/checkpoints/finetuned_model-epoch=00-train_loss=2.09.ckpt (deflated 22%)\n",
      "  adding: kaggle/working/checkpoints/finetuned_model-epoch=00-train_loss=2.14.ckpt (deflated 22%)\n",
      "  adding: kaggle/working/checkpoints/last-v1.ckpt (deflated 22%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r file.zip /kaggle/working/checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: Anime of the franchise \"Neon Genesis Evangelion\". This is the first anime in the franchise, and the series is set during the events of the movie \"Godzilla vs. Mecha Giga-Mars\". The anime is about the struggle of a group of survivors who survived the destruction of the Earth after the fall of mankind.\n",
      "\n",
      "(Source: Anime News Network)\n",
      "\n",
      "Description:\n",
      "An earth-shatter\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"mps\") \n",
    "\n",
    "model_path = \"Models/Anime_Model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "def generate_response(instruction, query, model, tokenizer, max_new_tokens=100):\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{query}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "instruction = \"Describe this anime\"\n",
    "query = \"Neon Genesis Evangelion\"\n",
    "\n",
    "response = generate_response(instruction, query, model, tokenizer)\n",
    "print(\"out:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6323581,
     "sourceId": 10227929,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
