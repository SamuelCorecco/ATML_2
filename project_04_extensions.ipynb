{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANIME PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:09:45.887438Z",
     "iopub.status.busy": "2025-01-07T21:09:45.887211Z",
     "iopub.status.idle": "2025-01-07T21:10:27.820704Z",
     "shell.execute_reply": "2025-01-07T21:10:27.819495Z",
     "shell.execute_reply.started": "2025-01-07T21:09:45.887416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install rouge-score\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install bitsandbytes\n",
    "!pip install lightning\n",
    "!pip install peft\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:10:27.822115Z",
     "iopub.status.busy": "2025-01-07T21:10:27.821874Z",
     "iopub.status.idle": "2025-01-07T21:10:45.531421Z",
     "shell.execute_reply": "2025-01-07T21:10:45.530764Z",
     "shell.execute_reply.started": "2025-01-07T21:10:27.822095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Double Training Process with QLoRA\n",
    "\n",
    "The workflow involves a **two-step training process** to fine-tune and specialize a quantized Transformer model using QLoRA (Quantized Low-Rank Adaptation). This approach combines a general fine-tuning step with a domain-specific customization step, ensuring both adaptability and precision.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Overview of the Training Steps\n",
    "\n",
    "### 1Ô∏è‚É£ **General Fine-Tuning**\n",
    "In the first stage, the base model is fine-tuned on `openassistant_best_replies_train.jsonl` to adapt it for conversational tasks:\n",
    "- **Dataset**: The data is formatted `### Human: <prompt> ### Assistant: <response>`.\n",
    "- **Goal**: Train the model to handle general chat-style interactions effectively.\n",
    "- **Techniques**:\n",
    "  - **4-Bit Quantization**: Reduces memory usage\n",
    "  - **LoRA (Low-Rank Adaptation)**: Adds trainable adapters to the model\n",
    "- **Outcome**: A fine-tuned model capable of generating coherent and contextually relevant responses in general conversation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Domain-Specific Fine-Tuning**\n",
    "In the second stage, the previously fine-tuned model is further trained on a custom dataset (`anime-dataset-2023.csv`) to specialize it for a specific domain (anime descriptions):\n",
    "- **Dataset**:\n",
    "  - Filtered to include only high-rated anime (scores ‚â• 8.3).\n",
    "  - Reformatted the dataset to have same format as before\n",
    "- **Goal**: Adapt the model to generate accurate and detailed domain-specific outputs.\n",
    "- **Techniques**:\n",
    "  - The fine-tuned model from the first step is reloaded.\n",
    "  - The Alpaca-style dataset ensures compatibility with the conversational format.\n",
    "  - The same QLoRA setup (4-bit quantization + LoRA) is used to enable efficient training on limited hardware.\n",
    "- **Outcome**: A specialized model capable of producing detailed and accurate descriptions of anime.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Benefits of the Double Training Process\n",
    "- **General Adaptability**: The first training step ensures the model is well-suited for general conversational tasks, providing a strong foundation.\n",
    "- **Domain Specialization**: The second training step allows the model to excel in a specific domain (anime descriptions) without forgetting its general conversational abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# STEP 2 Load quantizate model\n",
    "# ####################################\n",
    "\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "\n",
    "train_formatted = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "test_formatted = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"test\"], lines=True)\n",
    "train_dataset = Dataset.from_dict({\"text\": train_formatted[\"text\"].tolist()})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_formatted[\"text\"].tolist()})\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  \n",
    "    bnb_4bit_quant_type=\"nf4\",              \n",
    "    bnb_4bit_use_double_quant=True          \n",
    ")\n",
    "\n",
    "base_model_id = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",          \n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 3 LoRa\n",
    "# ####################################\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model) \n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        [sample['text'] for sample in batch], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    labels = inputs.input_ids.clone()\n",
    "    labels[~inputs.attention_mask.bool()] = -100\n",
    "    return inputs, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    collate_fn=collate_fn, \n",
    "    shuffle=True, \n",
    "    batch_size=2, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# just as we see in class\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "        # Shift logits to exclude the last element\n",
    "        # shift labels to exclude the first element\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        # Compute LM loss token-wise\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 4 Trainer + Train\n",
    "# ####################################\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints\",      \n",
    "    filename=\"model-{epoch:02d}-{train_loss:.2f}\", \n",
    "    save_top_k=3,                  \n",
    "    monitor=\"train_loss\",         \n",
    "    mode=\"min\",                  \n",
    "    save_weights_only=False   \n",
    ")\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=64,\n",
    "    precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=1,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 5 Save\n",
    "# ####################################\n",
    "\n",
    "model.save_pretrained(\"./finetuned_qlora_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_qlora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "We need to extract and prepare the dataset and build alpaca. <br>\n",
    "So we need to extract some information that we then want to get from the model i.e. Release Date, Gender and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:11:06.220039Z",
     "iopub.status.busy": "2025-01-07T21:11:06.219719Z",
     "iopub.status.idle": "2025-01-07T21:11:07.762582Z",
     "shell.execute_reply": "2025-01-07T21:11:07.761866Z",
     "shell.execute_reply.started": "2025-01-07T21:11:06.220007Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean len  1025.012\n",
      "number ex 250\n",
      "INSTRUCTION\n",
      "Describe this anime\n",
      "INPUT\n",
      "Describe this anime\n",
      "OUTPUT\n",
      "This anime was released on Apr 3, 1998 to Apr 24, 1999.\n",
      "Its genres are Action, Award Winning, Sci-Fi.\n",
      "The description of this anime is: Crime is timeless. By the year 2071, humanity has expanded across the galaxy, filling the surface of other planets with settlements like those on Earth. These new societies are plagued by murder, drug use, and theft, and intergalactic outlaws are hunted by a growing number of tough bounty hunters.  Spike Spiegel and Jet Black pursue criminals throughout space to make a humble living. Beneath his goofy and aloof demeanor, Spike is haunted by the weight of his violent past. Meanwhile, Jet manages his own troubled memories while taking care of Spike and the Bebop, their ship. The duo is joined by the beautiful con artist Faye Valentine, odd child Edward Wong Hau Pepelu Tivrusky IV, and Ein, a bioengineered Welsh Corgi.  While developing bonds and working to catch a colorful cast of criminals, the Bebop crew's lives are disrupted by a menace from Spike's past. As a rival's maniacal plot continues to unravel, Spike must choose between life with his newfound family or revenge for his old wounds.\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"/kaggle/input/myanimelist-dataset/anime-dataset-2023.csv\"\n",
    "\n",
    "anime_df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "def prepare_alpaca_format_strict(df):\n",
    "    tot_len = 0\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['Score'] == 'UNKNOWN':\n",
    "            continue\n",
    "        if float(row['Score']) < 8.3:\n",
    "            continue\n",
    "        instruction = \"Describe this anime\"\n",
    "        input_text = row['Name']\n",
    "        release_date = row['Aired']\n",
    "        genres = row['Genres']\n",
    "        synopsis = row['Synopsis']\n",
    "\n",
    "        if pd.isnull(input_text) or pd.isnull(release_date) or pd.isnull(genres) or pd.isnull(synopsis):\n",
    "            continue\n",
    "\n",
    "        synopsis = synopsis.replace(\"\\n\", \" \")\n",
    "\n",
    "        output_text = (f\"This anime was released on {release_date}.\\n\"\n",
    "                       f\"Its genres are {genres}.\\n\"\n",
    "                       f\"The description of this anime is: {synopsis}\")\n",
    "\n",
    "        tot_len += len(output_text)\n",
    "\n",
    "        example = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        }\n",
    "        examples.append(example)\n",
    "\n",
    "    print(\"mean len \", tot_len/len(examples))\n",
    "    print(\"number ex\", len(examples))\n",
    "    return examples\n",
    "\n",
    "alpaca_dataset_name_only = prepare_alpaca_format_strict(anime_df)\n",
    "\n",
    "print(\"INSTRUCTION\")\n",
    "print(alpaca_dataset_name_only[0][\"instruction\"])\n",
    "print(\"INPUT\")\n",
    "print(alpaca_dataset_name_only[0][\"instruction\"])\n",
    "print(\"OUTPUT\")\n",
    "print(alpaca_dataset_name_only[0][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:11:18.381284Z",
     "iopub.status.busy": "2025-01-07T21:11:18.380967Z",
     "iopub.status.idle": "2025-01-07T21:11:18.401768Z",
     "shell.execute_reply": "2025-01-07T21:11:18.400872Z",
     "shell.execute_reply.started": "2025-01-07T21:11:18.381252Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE:\n",
      "### Human: Describe this anime Cowboy Bebop ### Assistant: This anime was released on Apr 3, 1998 to Apr 24, 1999.\n",
      "Its genres are Action, Award Winning, Sci-Fi.\n",
      "The description of this anime is: Crime is timeless. By the year 2071, humanity has expanded across the galaxy, filling the surface of other planets with settlements like those on Earth. These new societies are plagued by murder, drug use, and theft, and intergalactic outlaws are hunted by a growing number of tough bounty hunters.  Spike Spiegel and Jet Black pursue criminals throughout space to make a humble living. Beneath his goofy and aloof demeanor, Spike is haunted by the weight of his violent past. Meanwhile, Jet manages his own troubled memories while taking care of Spike and the Bebop, their ship. The duo is joined by the beautiful con artist Faye Valentine, odd child Edward Wong Hau Pepelu Tivrusky IV, and Ein, a bioengineered Welsh Corgi.  While developing bonds and working to catch a colorful cast of criminals, the Bebop crew's lives are disrupted by a menace from Spike's past. As a rival's maniacal plot continues to unravel, Spike must choose between life with his newfound family or revenge for his old wounds.\n",
      "\n",
      "### Human: Describe this anime Neon Genesis Evangelion: The End of Evangelion ### Assistant: This anime was released on Jul 19, 1997.\n",
      "Its genres are Avant Garde, Drama, Sci-Fi.\n",
      "The description of this anime is: Shinji Ikari is left emotionally comatose after the death of a dear friend. With his son mentally unable to pilot the humanoid robot Evangelion Unit-01, Gendou Ikari's NERV races against the shadow organization SEELE to see who can enact their ultimate plan first. SEELE desires to create a godlike being by fusing their own souls into an Evangelion unit, while Gendou wishes to revert all of humanity into one primordial being so that he can be reunited with Yui, his deceased wife.  SEELE unleashes its military forces in a lethal invasion of NERV headquarters. As SEELE's forces cut down NERV's scientists and security personnel, Asuka Langley Souryuu pilots Evangelion Unit-02 in a desperate last stand against SEELE's heaviest weaponry.  The battle rages on, and a depressed Shinji hides deep within NERV's headquarters. With the fate of the world resting in Shinji's hands, Captain Misato Katsuragi hunts for the teenage boy as society crumbles around them.\n"
     ]
    }
   ],
   "source": [
    "alpaca_data = alpaca_dataset_name_only\n",
    "\n",
    "train_formatted = [\n",
    "    f\"### Human: {sample['instruction']} {sample['input']}\"\n",
    "    f\" ### Assistant: {sample['output']}\"\n",
    "    for sample in alpaca_data\n",
    "]\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_formatted})\n",
    "\n",
    "print(\"EXAMPLE:\")\n",
    "print(train_dataset[0]['text'])\n",
    "print(\"\")\n",
    "print(train_dataset[5]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:11:58.773279Z",
     "iopub.status.busy": "2025-01-07T21:11:58.772922Z",
     "iopub.status.idle": "2025-01-07T21:39:40.241170Z",
     "shell.execute_reply": "2025-01-07T21:39:40.240373Z",
     "shell.execute_reply.started": "2025-01-07T21:11:58.773246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096aef7962ee4368b6bfd05539c298e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6d1015c5ef433781df563dd6c71c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebeac5ee98841249bd1edc2ec81a09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 616 M  | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "615 M     Non-trainable params\n",
      "616 M     Total params\n",
      "2,466.947 Total estimated model params size (MB)\n",
      "757       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5cbead30b42a4bc1b7ebc0d7babfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./updated_finetuned_qlora_model/tokenizer_config.json',\n",
       " './updated_finetuned_qlora_model/special_tokens_map.json',\n",
       " './updated_finetuned_qlora_model/tokenizer.model',\n",
       " './updated_finetuned_qlora_model/added_tokens.json',\n",
       " './updated_finetuned_qlora_model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  \n",
    "else:\n",
    "    device = torch.device(\"cpu\") \n",
    "\n",
    "\n",
    "# ####################################\n",
    "# STEP 1: Reload model\n",
    "# ####################################\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  \n",
    "    bnb_4bit_quant_type=\"nf4\",              \n",
    "    bnb_4bit_use_double_quant=True          \n",
    ")\n",
    "\n",
    "finetuned_model_path = \"./finetuned_qlora_model\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    finetuned_model_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.gradient_checkpointing_enable()\n",
    "model.train()\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ####################################\n",
    "# STEP 2: Prepare DataLoader\n",
    "# ####################################\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        [sample['text'] for sample in batch], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=150\n",
    "    )\n",
    "    labels = inputs.input_ids.clone()\n",
    "    labels[~inputs.attention_mask.bool()] = -100\n",
    "    return inputs, labels\n",
    "    \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    batch_size=5,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# ####################################\n",
    "# STEP 3: Define Lightning Module\n",
    "# ####################################\n",
    "\n",
    "class LightningWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "lightning_model = LightningWrapper(model)\n",
    "\n",
    "# ####################################\n",
    "# STEP 4: Configure Trainer and Checkpoints\n",
    "# ####################################\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=8,\n",
    "    precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=25,\n",
    "    # callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ####################################\n",
    "# STEP 5: Train the Model\n",
    "# ####################################\n",
    "\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader)\n",
    "\n",
    "# ####################################\n",
    "# STEP 6: Save the Updated Model\n",
    "# ####################################\n",
    "\n",
    "model.save_pretrained(\"./updated_finetuned_qlora_model\")\n",
    "tokenizer.save_pretrained(\"./updated_finetuned_qlora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T21:49:24.881441Z",
     "iopub.status.busy": "2025-01-07T21:49:24.881066Z",
     "iopub.status.idle": "2025-01-07T21:49:47.514752Z",
     "shell.execute_reply": "2025-01-07T21:49:47.513891Z",
     "shell.execute_reply.started": "2025-01-07T21:49:24.881409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The anime was released on January 25, 1997 to March 30, 1998.\n",
      "Its genres are Drama, Sci-Fi.\n",
      "The description of this anime is: A group of scientists at a research facility in Tokyo are trying to develop the ultimate weapon, which will destroy humanity and wipe out all life on Earth. However, as they continue their work, they unknowingly awaken\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"./updated_finetuned_qlora_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "formatted_input = \"### Human: Describe this anime Neon Genesis Evangelion ### Assistant: \"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    formatted_input, \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = response.split(\"### Assistant:\")[-1].strip()\n",
    "response = response.split(\"###\")[0].strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Evangelion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model + Special Query + zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:07:20.045607Z",
     "iopub.status.busy": "2025-01-07T22:07:20.045278Z",
     "iopub.status.idle": "2025-01-07T22:07:42.213149Z",
     "shell.execute_reply": "2025-01-07T22:07:42.212298Z",
     "shell.execute_reply.started": "2025-01-07T22:07:20.045582Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animes de manga y televisi√≥n, cuyas pel√≠culas han sido adaptadas en todo el mundo. Est√° grabado por la BBC y fue dirigida por Gainax entre otros estudios. La trama sigue a un ser humano que viaja al futuro para encontrar una manera de detener un ataque cient√≠fico contra el planeta M-279. El personaje de Shinji Ikari es uno de los m√°s conocidos, pero\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "formatted_prompt = \"\"\"\n",
    "### Human: Describe this anime Neon Genesis Evangelion ### Assistant: \n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt, \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = response.split(\"### Assistant:\")[-1].strip()\n",
    "response = response.split(\"###\")[0].strip()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model + Normal Query + zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:06:34.927995Z",
     "iopub.status.busy": "2025-01-07T22:06:34.927695Z",
     "iopub.status.idle": "2025-01-07T22:06:57.476072Z",
     "shell.execute_reply": "2025-01-07T22:06:57.474905Z",
     "shell.execute_reply.started": "2025-01-07T22:06:34.927971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neon Genesis Evangelion is a Japanese science fiction film directed by Shinji Higuchi, written by Hideaki Anno and produced by Studio Ghibli. It was released in...\n",
      "What are some interesting facts about Akira?\n",
      "Akira Toriyama (Ê∏äÂêçÊòéÂÖ∏) is known for his work on Dragon Ball Z, Godzilla: King of Monsters, Attack on Titan, Naruto, and many other popular m\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "formatted_prompt = \"\"\"\n",
    "Please describe the anime Neon Genesis Evangelion\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt, \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = response.split(\"Please describe the anime Neon Genesis Evangelion\")[-1].strip()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model + Normal Query + few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:02:29.465234Z",
     "iopub.status.busy": "2025-01-07T22:02:29.464896Z",
     "iopub.status.idle": "2025-01-07T22:02:52.767302Z",
     "shell.execute_reply": "2025-01-07T22:02:52.766461Z",
     "shell.execute_reply.started": "2025-01-07T22:02:29.465186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime: Neon Genesis Evangelion\n",
      "Release Date: March 27, 1997\n",
      "Genres: Science Fiction, Thriller, Horror\n",
      "Description: A television special directed by ShinjiIkari that tells the story of three high school students who are transformed into angels after taking part in a military experiment gone wrong. The film was adapted into sixteen episodes, four of which were used in the television special's original release.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "formatted_prompt = \"\"\"\n",
    "Below is an example of how I describe anime series in a structured format.\n",
    "\n",
    "Anime: Cowboy Bebop\n",
    "Release Date: April 3, 1998\n",
    "Genres: Sci-Fi, Action, Space Western\n",
    "Description: Follows a group of bounty hunters traveling on the spaceship Bebop. Known for its genre-blending storytelling, memorable soundtrack, and stylish action sequences.\n",
    "\n",
    "Now please describe the anime \"Neon Genesis Evangelion\" in the same structured format.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt, \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = response.split(\"structured format.\")[-1].strip()\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3384322,
     "sourceId": 6207733,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
